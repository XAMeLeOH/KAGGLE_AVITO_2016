{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import cv2\n",
    "import hashlib\n",
    "import itertools\n",
    "import memcache\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "def get_image(x, mc, zipfiles):\n",
    "    f = mc.get(\"file:\"+x)\n",
    "    if f is not None:\n",
    "        return f\n",
    "    fbname = \"00\" + x\n",
    "    f = zipfiles[\"%01d\" % int(fbname[-2])].read(\"Images_%01d/%01d/%s.jpg\" % (int(fbname[-2]), int(fbname[-2:]), x))\n",
    "    mc.set(\"file:\"+x, f, time=10)\n",
    "    return f\n",
    "    \n",
    "def get_hash(x):\n",
    "    import hashlib\n",
    "    \n",
    "    return hashlib.md5(x).hexdigest()\n",
    "\n",
    "def get_cv2_hist(x):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    return cv2.normalize(\n",
    "        cv2.calcHist([\n",
    "                cv2.imdecode(np.frombuffer(x, np.uint8), 1)\n",
    "            ], [0, 1, 2], None, [8, 8, 8],\n",
    "                     [0, 256, 0, 256, 0, 256])\n",
    "    ).flatten()\n",
    "\n",
    "def get_skimg(x):\n",
    "    from StringIO import StringIO\n",
    "    from skimage.io import imread\n",
    "    \n",
    "    return imread(StringIO(x))\n",
    "\n",
    "def process_image_pair(args):\n",
    "    index, item1, item2, x, y = args\n",
    "    import sys\n",
    "    import warnings\n",
    "    from scipy.spatial import distance as dist\n",
    "    from skimage import color, exposure, measure\n",
    "    from skimage.feature import (match_descriptors, corner_harris,\n",
    "                             corner_peaks, BRIEF, ORB, plot_matches)\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    try:\n",
    "        result.append((\"Shape\", float(get_skimg(x).shape == get_skimg(y).shape)))\n",
    "    except:\n",
    "        result.append((\"Shape\", None))\n",
    "        sys.excepthook(*sys.exc_info())\n",
    "    \n",
    "    try:\n",
    "        result.append((\"Exact\", float(get_hash(x) == get_hash(y))))\n",
    "    except:\n",
    "        result.append((\"Exact\", None))\n",
    "        sys.excepthook(*sys.exc_info())\n",
    "    \n",
    "    try:\n",
    "        cv2histx = get_cv2_hist(x)\n",
    "        cv2histy = get_cv2_hist(y)\n",
    "    except:\n",
    "        sys.excepthook(*sys.exc_info())\n",
    "\n",
    "    OPENCV_METHODS = (\n",
    "        (\"Correlation\", cv2.cv.CV_COMP_CORREL),\n",
    "        (\"Chi-Squared\", cv2.cv.CV_COMP_CHISQR),\n",
    "        (\"Intersection\", cv2.cv.CV_COMP_INTERSECT),\n",
    "        (\"Hellinger\", cv2.cv.CV_COMP_BHATTACHARYYA))\n",
    "    for n, m in OPENCV_METHODS:\n",
    "        try:\n",
    "            result.append((n, round(cv2.compareHist(cv2histx, cv2histy, m), 6)))\n",
    "        except:\n",
    "            result.append((n, None))\n",
    "            sys.excepthook(*sys.exc_info())\n",
    "\n",
    "    SCIPY_METHODS = (\n",
    "        (\"scipy.braycurtis\", dist.braycurtis),\n",
    "        (\"scipy.canberra\", dist.canberra),\n",
    "        (\"scipy.chebyshev\", dist.chebyshev),\n",
    "        (\"scipy.cityblock\", dist.cityblock),\n",
    "        (\"scipy.cosine\", dist.cosine),\n",
    "#         (\"scipy.dice\", dist.dice),\n",
    "        (\"scipy.euclidean\", dist.euclidean),\n",
    "        (\"scipy.hamming\", dist.hamming),\n",
    "        (\"scipy.jaccard\", dist.jaccard),\n",
    "        (\"scipy.sqeuclidean\", dist.sqeuclidean),\n",
    "    )\n",
    "    for n, m in SCIPY_METHODS:\n",
    "        try:\n",
    "            result.append((n, round(m(cv2histx, cv2histy), 6)))\n",
    "        except:\n",
    "            result.append((n, None))\n",
    "            sys.excepthook(*sys.exc_info())\n",
    "\n",
    "    try:\n",
    "        skimgx = get_skimg(x)\n",
    "        skimgy = get_skimg(y)\n",
    "    except:\n",
    "        sys.excepthook(*sys.exc_info())\n",
    "        \n",
    "    try:\n",
    "        skimgxa = exposure.equalize_adapthist(color.rgb2gray(skimgx), clip_limit=0.01)\n",
    "        skimgya = exposure.equalize_adapthist(color.rgb2gray(skimgy), clip_limit=0.01)\n",
    "    except:\n",
    "        sys.excepthook(*sys.exc_info())\n",
    "\n",
    "    SKIMAGE_METHODS = (\n",
    "        (\"skimage.compare_mse\", measure.compare_mse),\n",
    "        (\"skimage.compare_ssim_3\", lambda x, y: measure.compare_ssim(x, y, win_size=3, multichannel=True)),\n",
    "        (\"skimage.compare_ssim_5\", lambda x, y: measure.compare_ssim(x, y, win_size=5, multichannel=True)),\n",
    "        (\"skimage.compare_ssim_7\", lambda x, y: measure.compare_ssim(x, y, win_size=7, multichannel=True)),\n",
    "    )\n",
    "            \n",
    "    for n, m in SKIMAGE_METHODS:\n",
    "        try:\n",
    "            if result[0][1]:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    result.append((n, round(m(skimgx, skimgy), 6)))\n",
    "            else:\n",
    "                result.append((n, None))\n",
    "        except:\n",
    "            result.append((n, None))\n",
    "            sys.excepthook(*sys.exc_info())\n",
    "\n",
    "    SKIMAGE_METHODS = (\n",
    "        (\"skimage.compare_mse.adapt\", measure.compare_mse),\n",
    "        (\"skimage.compare_ssim_3.adapt\", lambda x, y: measure.compare_ssim(x, y, win_size=3)),\n",
    "        (\"skimage.compare_ssim_5.adapt\", lambda x, y: measure.compare_ssim(x, y, win_size=5)),\n",
    "        (\"skimage.compare_ssim_7.adapt\", lambda x, y: measure.compare_ssim(x, y, win_size=7)),\n",
    "    )\n",
    "            \n",
    "    for n, m in SKIMAGE_METHODS:\n",
    "        try:\n",
    "            if result[0][1]:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    result.append((n, round(m(skimgxa, skimgya), 6)))\n",
    "            else:\n",
    "                result.append((n, None))\n",
    "        except:\n",
    "            result.append((n, None))\n",
    "            sys.excepthook(*sys.exc_info())\n",
    "            \n",
    "#     try:\n",
    "#         descriptor_extractor = ORB(n_keypoints=200)\n",
    "#         descriptor_extractor.detect_and_extract(skimgxa)\n",
    "#         keypoints1 = descriptor_extractor.keypoints\n",
    "#         descriptors1 = descriptor_extractor.descriptors\n",
    "#         descriptor_extractor.detect_and_extract(skimgya)\n",
    "#         keypoints2 = descriptor_extractor.keypoints\n",
    "#         descriptors2 = descriptor_extractor.descriptors\n",
    "#         result.append((\"ORB\", float(len(match_descriptors(descriptors1, descriptors2, cross_check=True)))))\n",
    "#     except:\n",
    "#         result.append((\"ORB\", None))\n",
    "#         sys.excepthook(*sys.exc_info())\n",
    "    \n",
    "#     try:\n",
    "#         keypoints1 = corner_peaks(corner_harris(skimgxa), min_distance=5)\n",
    "#         keypoints2 = corner_peaks(corner_harris(skimgya), min_distance=5)\n",
    "#         extractor = BRIEF()\n",
    "#         extractor.extract(skimgxa, keypoints1)\n",
    "#         keypoints1 = keypoints1[extractor.mask]\n",
    "#         descriptors1 = extractor.descriptors\n",
    "#         extractor.extract(skimgya, keypoints2)\n",
    "#         keypoints2 = keypoints2[extractor.mask]\n",
    "#         descriptors2 = extractor.descriptors\n",
    "#         result.append((\"BRIEF\", float(len(match_descriptors(descriptors1, descriptors2, cross_check=True)))))\n",
    "#     except:\n",
    "#         result.append((\"BRIEF\", None))\n",
    "#         sys.excepthook(*sys.exc_info())\n",
    "    \n",
    "    return index, item1, item2, result\n",
    "\n",
    "\n",
    "def process_images_arrays(index, data, mc, zipfiles):\n",
    "    results = []\n",
    "    itemID_1, itemID_2, imgsx, imgsy = data\n",
    "    for imgx, imgy in itertools.product(str(imgsx).split(\", \"), str(imgsy).split(\", \")):\n",
    "        if all([imgx.strip(), imgy.strip()]):\n",
    "            results.append((imgx, imgy, process_image_pair(imgx, imgy, mc, zipfiles)))\n",
    "    return index, data, results\n",
    "\n",
    "class DumpArchive(object):\n",
    "    def __init__(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            raise Exception(\"Path %s does not exists\" % path)\n",
    "        self.path = path\n",
    "\n",
    "    def read(self, filename):\n",
    "        return open(os.path.join(self.path, filename), \"rb\").read()\n",
    "\n",
    "def process_images(data_dir=\".\"):\n",
    "    from zipfile import ZipFile\n",
    "    import csv\n",
    "    import functools\n",
    "    import numpy as np\n",
    "    import multiprocessing as mp\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "    \n",
    "    pool = mp.Pool(8)\n",
    "    mc = memcache.Client(['127.0.0.1:11211'], debug=0)\n",
    "    for mode in [\"train\", \"test\"]:\n",
    "        fn = os.path.join(data_dir, \"ItemPairs_%s.csv_ImgCmp.csv\" % mode)\n",
    "        oldindex = None\n",
    "        if os.path.exists(fn):\n",
    "            continue\n",
    "            oldindex = np.unique(pd.read_csv(fn)['id'].values)\n",
    "        params = {}\n",
    "        zipfiles = {}\n",
    "        for _ in xrange(10):\n",
    "            sys.stderr.write(str(_) + \" \")\n",
    "            try:\n",
    "                zipfiles[str(_)] = ZipFile(os.path.join(data_dir, \"Images_%d.zip\" % _))\n",
    "            except:\n",
    "                zipfiles[str(_)] = DumpArchive(data_dir)\n",
    "        sys.stderr.write(\"\\n\")\n",
    "        sys.stderr.write(fn + \"\\n\")\n",
    "        if mode == \"test\":\n",
    "            params[\"index_col\"] = 0\n",
    "        imgarr = pd.read_csv(os.path.join(data_dir, \"ItemInfo_%s.csv_images_array.csv\" % mode), index_col=0)\n",
    "        pr = pd.read_csv(os.path.join(data_dir, \"ItemPairs_%s.csv\" % mode), **params)\n",
    "        pr = pd.merge(pr, imgarr, left_on=\"itemID_1\", right_index=True, how=\"inner\", sort=False)\n",
    "        pr = pd.merge(pr, imgarr, left_on=\"itemID_2\", right_index=True, how=\"inner\", sort=False)\n",
    "        headers = None\n",
    "        if oldindex is not None:\n",
    "            print \"Drop indexes\", oldindex\n",
    "            pr.drop(oldindex, inplace=True)\n",
    "            headers = True\n",
    "        with open(fn, \"a\") as fo:\n",
    "            w = csv.writer(fo)\n",
    "            for index, itemID_1, itemID_2, res in pool.imap(process_image_pair, (\n",
    "                (index, itemID_1, itemID_2, get_image(imgx, mc, zipfiles), get_image(imgy, mc, zipfiles))\n",
    "                for index, (itemID_1, itemID_2, imgsx, imgsy) in itertools.izip(pr.index, iter(pr[[\"itemID_1\", \"itemID_2\", \"images_array_x\", \"images_array_y\"]].fillna(\"\").values))\n",
    "                for imgx, imgy in itertools.product(str(imgsx).split(\", \"), str(imgsy).split(\", \"))\n",
    "                if all([imgx.strip(), imgy.strip()])\n",
    "            )):\n",
    "                if headers is None:\n",
    "                    w.writerow([\"id\", \"itemID_1\", \"itemID_2\"] + [n for n, r in res])\n",
    "                    headers = True\n",
    "                w.writerow([index, itemID_1, itemID_2] + [r for n, r in res])\n",
    "                \n",
    "                \n",
    "#             for prindex, (itemID_1, itemID_2, imgsx, imgsy), results in itertools.imap(lambda args: process_images_arrays(args[0], args[1], mc, zipfiles), itertools.izip(pr.index, iter(pr[[\"itemID_1\", \"itemID_2\", \"images_array_x\", \"images_array_y\"]].fillna(\"\").values))):\n",
    "#                 print prindex\n",
    "#                 for imgx, imgy, res in results:\n",
    "#                     if headers is None:\n",
    "#                         w.writerow([\"id\", \"itemID_1\", \"itemID_2\"] + [n for n, r in res])\n",
    "#                         headers = True\n",
    "#                     w.writerow([prindex, itemID_1, itemID_2] + [r for n, r in res])\n",
    "                    \n",
    "#             for prindex, (itemID_1, itemID_2, imgsx, imgsy) in itertools.izip(pr.index, iter(pr[[\"itemID_1\", \"itemID_2\", \"images_array_x\", \"images_array_y\"]].fillna(\"\").values)):\n",
    "#                 print prindex\n",
    "#                 for imgx, imgy in itertools.product(str(imgsx).split(\", \"), str(imgsy).split(\", \")):\n",
    "#                     if all([imgx.strip(), imgy.strip()]):\n",
    "#                         res = process_image_pair(imgx, imgy, mc, zipfiles)\n",
    "#                         if headers is None:\n",
    "#                             w.writerow([\"id\", \"itemID_1\", \"itemID_2\"] + [n for n, r in res])\n",
    "#                             headers = True\n",
    "#                         w.writerow([prindex, itemID_1, itemID_2] + [r for n, r in res])\n",
    "        sys.stderr.write(\"DONE\\n\")\n",
    "            \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "analyzer = CountVectorizer(\n",
    "    min_df=10\n",
    ").build_analyzer()\n",
    "analyzerstop = CountVectorizer(\n",
    "    min_df=10,\n",
    "    stop_words=u'ищ прода прод из по от под все вс прод для эт нов м² ког ту том ком ко ещ еш тоб тольк ве тем во нет а быт ты и тех не на но при чег один вы ем ел ед ег те та кто то же будьт всю вся себ всех ним одн им из будеш нам будет у могут я нас так наш мно всег всем будут них мне будуч все могл когд мен теб он буд об для кем бы тот мож мог в к уж о чем вот тог моч их может ил можеш эт сво был нег нем от чтоб мо ест соб до да вас е с по вам что за есл как сам котор этот мы'.split(' ')\n",
    ").build_analyzer()\n",
    "stemmer = nltk.stem.snowball.RussianStemmer()\n",
    "\n",
    "translate = {\n",
    "    u\"а\": \"a\",\n",
    "    u\"б\": \"b\",\n",
    "    u\"в\": \"v\",\n",
    "    u\"г\": \"g\",\n",
    "    u\"д\": \"d\",\n",
    "    u\"е\": \"e\",\n",
    "    u\"ё\": \"e\",\n",
    "    u\"ж\": \"z\",\n",
    "    u\"з\": \"z\",\n",
    "    u\"и\": \"i\",\n",
    "    u\"й\": \"i\",\n",
    "    u\"к\": \"k\",\n",
    "    u\"л\": \"l\",\n",
    "    u\"м\": \"m\",\n",
    "    u\"н\": \"n\",\n",
    "    u\"о\": \"o\",\n",
    "    u\"п\": \"p\",\n",
    "    u\"р\": \"r\",\n",
    "    u\"с\": \"s\",\n",
    "    u\"т\": \"t\",\n",
    "    u\"у\": \"u\",\n",
    "    u\"ф\": \"f\",\n",
    "    u\"х\": \"h\",\n",
    "    u\"ц\": \"c\",\n",
    "    u\"ч\": \"c\",\n",
    "    u\"ш\": \"s\",\n",
    "    u\"щ\": \"s\",\n",
    "    u\"ъ\": \"\",\n",
    "    u\"ы\": \"i\",\n",
    "    u\"ь\": \"\",\n",
    "    u\"э\": \"e\",\n",
    "    u\"ю\": \"u\",\n",
    "    u\"я\": \"a\",\n",
    "}\n",
    "\n",
    "def stemstring(x):\n",
    "    return \" \".join([stemmer.stem(_).encode('utf8') for _ in analyzer(x)])\n",
    "    import re\n",
    "    resplit = re.compile(r'[a-z0-9]*[0-9][a-z0-9]*')\n",
    "    rechar = re.compile(r'[a-z]+')\n",
    "    renum = re.compile(r'[0-9]+')\n",
    "    res = []\n",
    "    wordnot = False\n",
    "    for _ in analyzer(x):\n",
    "        if _ == u\"не\" or _ == u\"ни\":\n",
    "            wordnot = True\n",
    "            continue\n",
    "        if wordnot:\n",
    "            _ = u\"не\" + unicode(_)\n",
    "            wordnot = False\n",
    "        splitted = re.split(resplit, _)\n",
    "        for __ in splitted:\n",
    "            if len(__) < 2:\n",
    "                continue\n",
    "            __ = stemmer.stem(__)\n",
    "            for sym in translate:\n",
    "                __ = __.replace(sym, translate[sym])\n",
    "            __ = re.sub(r'(?u)([^\\W\\d])\\1+', r'\\1', __)\n",
    "#             for i in range(len(__)-3):\n",
    "#                 res.append(__[i:i+4])\n",
    "#             if len(__) < 4:\n",
    "#                 res.append(__)\n",
    "            res.append(__)\n",
    "        for __ in re.findall(rechar, _):\n",
    "            res.append(__)\n",
    "        for __ in re.findall(renum, _):\n",
    "            res.append(__)\n",
    "    return unicode(\" \".join(res)).encode(\"UTF-8\")\n",
    "\n",
    "\n",
    "def stemstringstop(x):\n",
    "    return \" \".join([stemmer.stem(_).encode('utf8') for _ in analyzerstop(x)])\n",
    "\n",
    "\n",
    "def attrsJsonDecode(x):\n",
    "    try:\n",
    "        x = json.loads(x)\n",
    "    except:\n",
    "        return x\n",
    "    res = {}\n",
    "    for k, v in x.iteritems():\n",
    "        if v.startswith('\"') and '\"=>\"' in v:\n",
    "            subres = {}\n",
    "            V = json.loads('{' + v.replace('\"=>\"', '\": \"').replace('//\"', '\\\\\"').replace('////', '//') + '}')\n",
    "            for k1 in sorted(V.keys()):\n",
    "                if V[k1].startswith('\"') and '\"=>\"' in v:\n",
    "                    try:\n",
    "                        w = json.loads('{' + V[k1].replace('\"=>\"', '\": \"').replace('//\"', '\\\\\"').replace('////', '//') + '}')\n",
    "                    except:\n",
    "                        print V[k1]\n",
    "                        continue\n",
    "                    for k2, v2 in w.iteritems():\n",
    "                        if \"-\".join([k, k2]) not in subres:\n",
    "                            subres[\"-\".join([k, k2])] = {\n",
    "                                k1: stemstring(v2),\n",
    "                            }\n",
    "                        else:\n",
    "                            subres[\"-\".join([k, k2])][k1] = stemstring(v2)\n",
    "                else:\n",
    "                    subres[\"-\".join([k, k1])] = stemstring(V[k1])\n",
    "            res.update(subres)\n",
    "        else:\n",
    "            res[k] = v\n",
    "    return json.dumps(res)\n",
    "\n",
    "\n",
    "def split_files(data_dir=\".\"):\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "    from sklearn.externals.joblib import Parallel, delayed\n",
    "    for fn in [\"ItemInfo_train.csv\", \"ItemInfo_test.csv\"]:\n",
    "        c = None\n",
    "        columns = \"categoryID,parentCategoryID,title,description,images_array,attrsJSON,price,locationID,regionID,metroID,lat,lon,stoptitle,stopdescription\".split(\",\")\n",
    "        for colname in columns:\n",
    "            colfn = \"/\".join([data_dir, fn + \"_\" + colname + \".csv\"])\n",
    "            if os.path.exists(colfn):\n",
    "                continue\n",
    "            sys.stderr.write(fn + \": \" + colname)\n",
    "            if c is None:\n",
    "                c = pd.read_csv(data_dir + \"/\" + fn, index_col=0)\n",
    "                c = c.join(pd.read_csv(data_dir + \"/Category.csv\", index_col=0), on=\"categoryID\")\n",
    "                c = c.join(pd.read_csv(data_dir + \"/Location.csv\", index_col=0), on=\"locationID\")\n",
    "            if colname in [\"title\", \"description\"]:\n",
    "                c[colname] = Parallel(-1)(delayed(stemstring)(_) for _ in c[colname].fillna(\"\").values)\n",
    "                c[[colname]].to_csv(colfn)\n",
    "            if colname in [\"stoptitle\", \"stopdescription\"]:\n",
    "                c[colname] = Parallel(-1)(delayed(stemstringstop)(_) for _ in c[colname.replace(\"stop\", \"\")].fillna(\"\").values)\n",
    "                c[[colname]].to_csv(colfn)\n",
    "            elif colname in [\"attrsJSON\"]:\n",
    "                c[colname] = Parallel(-1)(delayed(attrsJsonDecode)(_) for _ in c[colname].fillna(\"\").values)\n",
    "                c[[colname]].to_csv(colfn)\n",
    "            else:\n",
    "                c[[colname]].to_csv(colfn)\n",
    "            sys.stderr.write(\" DONE\\n\")\n",
    "\n",
    "split_files(\"data\")\n",
    "process_images(\"data\")\n",
    "\n",
    "def printreturn(x):\n",
    "    print x\n",
    "    return x\n",
    "\n",
    "def split_pair_files(data_dir=\".\"):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sys\n",
    "    from sklearn.externals.joblib import Parallel, delayed\n",
    "    for mode in [\"train\", \"test\"]:\n",
    "        fn = \"ItemPairs_%s.csv\" % mode\n",
    "        c = None\n",
    "        imcmp = None\n",
    "        columns = [\n",
    "            (\"imagesdist_Exact\", lambda x: x[x[\"Exact\"] > 0.95].index),\n",
    "        ] + [\n",
    "            (\"imagesdist_Correlation_%d\" % _, lambda x, t=_: x[x[\"Correlation\"] > (0.01 * t)].index)\n",
    "            for _ in range(99, 84, -1)\n",
    "        ] + [\n",
    "            (\"imagesdist_Chi-Squared_%d\" % _, lambda x, t=_: x[x[\"Chi-Squared\"] < (0.01 * t)].index)\n",
    "            for _ in range(1, 16)\n",
    "        ] + [\n",
    "            (\"imagesdist_Intersection_%d\" % _, lambda x, t=_: x[x[\"Intersection\"] > (0.01 * t)].index)\n",
    "            for _ in range(250, 150, -10)\n",
    "        ] + [\n",
    "            (\"imagesdist_Hellinger_%d\" % _, lambda x, t=_: x[x[\"Hellinger\"] < (0.01 * t)].index)\n",
    "            for _ in range(1, 11)\n",
    "        ]\n",
    "        for colname, fltr in columns:\n",
    "            colfn = \"/\".join([data_dir, fn + \"_\" + colname + \".csv\"])\n",
    "            if os.path.exists(colfn):\n",
    "                continue\n",
    "            sys.stderr.write(fn + \": \" + colname)\n",
    "            \n",
    "            if c is None:\n",
    "                imfn = (\"data/ItemInfo_%s.csv\" % mode) + \"_images_array.csv\"\n",
    "                imcolumn = pd.read_csv(imfn, index_col=0).fillna(\"\").applymap(lambda x: [_ for _ in str(x).split(\", \") if _ != \"\"])\n",
    "\n",
    "                params = {}\n",
    "                if mode == \"test\":\n",
    "                    params[\"index_col\"] = 0\n",
    "                c = pd.read_csv(data_dir + \"/\" + fn, **params)\n",
    "                c = c.merge(imcolumn[[\"images_array\"]], how=\"left\", left_on=\"itemID_1\", right_index=True, sort=False)\n",
    "                c = c.merge(imcolumn[[\"images_array\"]], how=\"left\", left_on=\"itemID_2\", right_index=True, sort=False)\n",
    "                del imcolumn\n",
    "\n",
    "            if imcmp is None:\n",
    "                imcmp = pd.read_csv(\"/\".join([data_dir, \"ItemPairs_%s.csv_Images.csv\" % mode]), index_col=0)\n",
    "            \n",
    "            idlist = fltr(imcmp)\n",
    "            \n",
    "            def dist(x, y):\n",
    "                z = max(len(x), len(y))\n",
    "                if z == 0:\n",
    "                    return 0.0\n",
    "                m = {}\n",
    "                for i in x:\n",
    "                    for j in [_ for _ in y if _ not in m]:\n",
    "                        if \"|\".join([i, j]) in idlist or \"|\".join([j, i]) in idlist:\n",
    "                            m[j] = i\n",
    "                return 1.0 * (z - len(m)) / z\n",
    "            \n",
    "            c[colname] = np.frompyfunc(dist, 2, 1)(c[\"images_array_x\"].values, c[\"images_array_y\"].values)\n",
    "            \n",
    "            c[[colname]].to_csv(colfn, index_label=\"id\")\n",
    "            c.drop(colname, axis=1, inplace=True)\n",
    "            sys.stderr.write(\" DONE\\n\")\n",
    "\n",
    "        columns = [\n",
    "            ('imagesdist_%s' % field, field)\n",
    "            for field in (\n",
    "                'Correlation',\n",
    "                'Chi-Squared',\n",
    "                'Intersection',\n",
    "                'Hellinger',\n",
    "                'scipy.braycurtis',\n",
    "                'scipy.canberra',\n",
    "                'scipy.chebyshev',\n",
    "                'scipy.cityblock',\n",
    "                'scipy.cosine',\n",
    "                'scipy.euclidean',\n",
    "                'scipy.hamming',\n",
    "                'scipy.sqeuclidean',\n",
    "                'skimage.compare_mse',\n",
    "                'skimage.compare_ssim_3',\n",
    "                'skimage.compare_ssim_5',\n",
    "                'skimage.compare_ssim_7',\n",
    "                'skimage.compare_mse.adapt',\n",
    "                'skimage.compare_ssim_3.adapt',\n",
    "                'skimage.compare_ssim_5.adapt',\n",
    "                'skimage.compare_ssim_7.adapt',\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        if any((\n",
    "                not os.path.exists(\"/\".join([data_dir, \"%s_%s_%s.csv\" % (fn, column[0], func)]))\n",
    "                for column in columns\n",
    "                for func in ('min', 'mean', 'max')\n",
    "            )):\n",
    "\n",
    "            for colname, field in columns:\n",
    "            \n",
    "                sys.stderr.write(fn + \": \" + field + \": \")\n",
    "                imcmp = pd.read_csv(\"/\".join([data_dir, \"ItemPairs_%s.csv_ImgCmp.csv\" % mode]), usecols=['id', field])\n",
    "\n",
    "                for func in 'min', 'mean', 'max':\n",
    "                    sys.stderr.write(func + \" \")\n",
    "                    groupped = getattr(imcmp.groupby(\"id\"), func)()\n",
    "                    groupped['_'.join([\"imagesdist\", field, func])] = groupped[field]\n",
    "                    groupped[['_'.join([\"imagesdist\", field, func])]].to_csv(\"/\".join([data_dir, \"%s_%s_%s.csv\" % (fn, colname, func)]))\n",
    "                sys.stderr.write(\"  DONE\\n\")\n",
    "\n",
    "split_pair_files(\"data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "from scipy import sparse as sp\n",
    "\n",
    "def bitwise_op(x, y, op):\n",
    "    assert x.shape == y.shape\n",
    "    x = x.astype(bool).astype(int)\n",
    "    y = y.astype(bool).astype(int)\n",
    "    if op == \"and\":\n",
    "        z = (x+y) > 1\n",
    "    elif op == \"or\":\n",
    "        z = (x+y) > 0\n",
    "    elif op == \"xor\":\n",
    "        z = ((x+y) == 1)\n",
    "    else:\n",
    "        raise Exception(\"Unknown operation `%s`\" % op)\n",
    "    return z.astype(float).tocsr()\n",
    "\n",
    "\n",
    "def preprocess_price(X):\n",
    "    import math\n",
    "    import numpy as np\n",
    "    def round_significant(x, digits):\n",
    "        return round(x, -int(math.log10(abs(x)) - digits + 1)) if abs(x) > 0.0 else 0.0\n",
    "    X[X[\"price\"] > 3000000] = 3000000\n",
    "    X[\"price\"] = X[\"price\"].map(lambda x: round_significant(x, 1) if np.isfinite(x) else x)\n",
    "    X[\"price\"] = X[\"price\"].map(lambda x: x if not np.isfinite(x) else 100.0 if x<100 else x)\n",
    "    X[\"price\"] = X[\"price\"].fillna(-1.0)\n",
    "    return X\n",
    "\n",
    "\n",
    "def preprocess_pair_price(X):\n",
    "    X.loc[X[\"price_x\"] == -1.0, \"price_x\"] = X[\"price_y\"]\n",
    "    X.loc[X[\"price_y\"] == -1.0, \"price_y\"] = X[\"price_x\"]\n",
    "    return X\n",
    "\n",
    "\n",
    "def getX(pr, data_dir=\".\", filename=\"ItemInfo_train.csv\", columns=None):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    X = pr[[\"itemID_1\", \"itemID_2\"]].copy()\n",
    "    for colname in columns:\n",
    "        colfn = \"/\".join([data_dir, filename + \"_\" + colname + \".csv\"])\n",
    "        column = pd.read_csv(colfn, index_col=0)\n",
    "        if colname == \"price\":\n",
    "            column = preprocess_price(column)\n",
    "        X = X.merge(column[[colname]], how=\"left\", left_on=\"itemID_1\", right_index=True, sort=False)\n",
    "        X = X.merge(column[[colname]], how=\"left\", left_on=\"itemID_2\", right_index=True, sort=False)\n",
    "        if colname == \"price\":\n",
    "            X = preprocess_pair_price(X)\n",
    "        del column\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import gc\n",
    "import gzip\n",
    "import editdistance\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance as dld\n",
    "from scipy import sparse as sp\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.externals.joblib import Parallel, cpu_count, delayed\n",
    "from sklearn.externals.joblib import Memory, dump, load\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics.pairwise import distance_metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.metaestimators import if_delegate_has_method\n",
    "from types import MethodType\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "memory = Memory(cachedir=os.getenv(\"CACHEDIR\", \"data/avito-cache\"))\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def getparams(est, *args, **kwargs):\n",
    "    e = clone(est)\n",
    "    e.fit(*args, **kwargs)\n",
    "    return {k: v for k, v in e.__dict__.iteritems() if re.search(r'[^_]_$', k)}\n",
    "\n",
    "def cached(obj):\n",
    "    obj.transform = memory.cache(obj.transform)\n",
    "    def fit(self, *args, **kwargs):\n",
    "        print \"fit\"\n",
    "        d = getparams(self, *args, **kwargs)\n",
    "        self.__dict__.update(d)\n",
    "        return self\n",
    "    setattr(obj, \"fit\", MethodType(fit, obj, obj.__class__))\n",
    "#     obj.fit = MethodType(fit, obj, obj.__class__)\n",
    "    return obj\n",
    "\n",
    "\n",
    "class Cached(BaseEstimator):\n",
    "    \n",
    "    @classmethod\n",
    "    def baseinit(cls):\n",
    "        import atexit\n",
    "        import os\n",
    "        import uuid\n",
    "        cls.base_dir = os.getenv(\"CACHEDIR\", \"data/avito-cache\") + \"/Cached-\" + str(uuid.uuid4())\n",
    "        os.mkdir(cls.base_dir)\n",
    "        atexit.register(cls.basedel)\n",
    "    \n",
    "    @classmethod\n",
    "    def basedel(cls):\n",
    "        import shutil\n",
    "        try:\n",
    "            shutil.rmtree(cls.base_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def __init__(self, est):\n",
    "        self.est = est\n",
    "    \n",
    "    def _load(self):\n",
    "        from sklearn.externals.joblib import load\n",
    "        self.est_ = load(self.__class__.base_dir + \"/\" + self.uuid_)\n",
    "\n",
    "    def _dump(self):\n",
    "        import uuid\n",
    "        from sklearn.externals.joblib import dump\n",
    "        self.uuid_ = \"joblib-\" + str(uuid.uuid4()) + \".est\"\n",
    "        dump(self.est_, self.__class__.base_dir + \"/\" + self.uuid_)\n",
    "        del self.est_\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        from sklearn.base import clone\n",
    "        self.est_ = clone(self.est)\n",
    "        if y is None:\n",
    "            self.est_ = memory.cache(self.est_.fit)(X)\n",
    "        else:\n",
    "            self.est_ = memory.cache(self.est_.fit)(X, y)\n",
    "        self._dump()\n",
    "        return self\n",
    "\n",
    "    @if_delegate_has_method(delegate=\"est\")\n",
    "    def transform(self, X):\n",
    "        self._load()\n",
    "        res = memory.cache(self.est_.transform)(X)\n",
    "        del self.est_\n",
    "        return res\n",
    "\n",
    "    @if_delegate_has_method(delegate=\"est\")\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "    @if_delegate_has_method(delegate=\"est\")\n",
    "    def predict(self, X):\n",
    "        self._load()\n",
    "        res = memory.cache(self.est_.predict)(X)\n",
    "        del self.est_\n",
    "        return res\n",
    "\n",
    "    @if_delegate_has_method(delegate=\"est\")\n",
    "    def predict_proba(self, X):\n",
    "        self._load()\n",
    "        res = memory.cache(self.est_.predict_proba)(X)\n",
    "        del self.est_\n",
    "        return res\n",
    "\n",
    "    @if_delegate_has_method(delegate=\"est\")\n",
    "    def predict_log_proba(self, X):\n",
    "        self._load()\n",
    "        res = memory.cache(self.est_.predict_log_proba)(X)\n",
    "        del self.est_\n",
    "        return res\n",
    "Cached.baseinit()\n",
    "\n",
    "\n",
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, include=None, exclude=None):\n",
    "        self.include = include\n",
    "        self.exclude = exclude\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.include is not None:\n",
    "            return X[:,self.include]\n",
    "        elif self.exclude is not None:\n",
    "            return X[:, [_ for _ in xrange(X.shape[1]) if _ not in self.exclude]]\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "class JSONAttr(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        super(JSONAttr, self).__init__()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return sp.hstack(\n",
    "            [\n",
    "                FeatureHasher(input_type=\"string\").fit_transform(\n",
    "                    (\n",
    "                        [\"___\".join(__.split(\": \")) for __ in _.strip(\"{}\").split(\", \")]\n",
    "                        for _ in iter(X[:,i]))\n",
    "                ).astype(float)\n",
    "                for i in range(X.shape[1])]\n",
    "        )\n",
    "\n",
    "class SwitchTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, est, postaction=None, preaction=None):\n",
    "        super(SwitchTransformer, self).__init__()\n",
    "        self.est = est\n",
    "        self.postaction = postaction\n",
    "        self.preaction = preaction\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print X.shape\n",
    "        self.est.fit(self._preprocess(X.reshape(-1,1)))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xs = self._postprocess([self.est.transform(self._preprocess(X[:,[i]])) for i in xrange(X.shape[1])])\n",
    "        print Xs.shape\n",
    "        return Xs\n",
    "\n",
    "    def _preprocess(self, X):\n",
    "        if self.preaction == \"ravel\":\n",
    "            return X.ravel()\n",
    "        return self.preaction(X) if self.preaction is not None else X\n",
    "\n",
    "    def _postprocess(self, Xs):\n",
    "        if self.postaction == \"diff\":\n",
    "            Xs = Xs[0] - Xs[1]\n",
    "        elif self.postaction in (\"and\", \"or\", \"xor\"):\n",
    "            Xs = bitwise_op(Xs[0], Xs[1], self.postaction)\n",
    "        elif isinstance(self.postaction, list):\n",
    "            Xs = [\n",
    "                bitwise_op(Xs[0], Xs[1], _)\n",
    "                for _ in self.postaction\n",
    "            ]\n",
    "            if any(sp.issparse(f) for f in Xs):\n",
    "                Xs = sp.hstack(Xs).tocsr()\n",
    "            else:\n",
    "                Xs = np.hstack(Xs)\n",
    "        elif self.postaction is not None:\n",
    "            Xs = self.postaction(Xs)\n",
    "        else:\n",
    "            if any(sp.issparse(f) for f in Xs):\n",
    "                Xs = sp.hstack(Xs).tocsr()\n",
    "            else:\n",
    "                Xs = np.hstack(Xs)\n",
    "        return Xs\n",
    "    \n",
    "class SupervisedDecision(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, est, method=None):\n",
    "        super(SupervisedDecision, self).__init__()\n",
    "        self.est = est\n",
    "        self.method = method\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.est.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method is None:\n",
    "            res = self.est.predict_proba(X)[:,[1]]\n",
    "        else:\n",
    "            res = getattr(self.est, self.method)(X)\n",
    "        if len(res.shape) == 1:\n",
    "            res = res.reshape(-1,1)\n",
    "        return res\n",
    "\n",
    "def _estimator_fit(est, X, y=None):\n",
    "    return est.fit(X, y)\n",
    "    \n",
    "def _estimator_transform(est, X):\n",
    "    return est.transform(X)\n",
    "    \n",
    "def _estimator_predict_proba(est, X):\n",
    "    return est.predict_proba(X)[:,1]\n",
    "\n",
    "class FilteredSupervisedDecision(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, est, switch, val, xcols):\n",
    "        super(FilteredSupervisedDecision, self).__init__()\n",
    "        self.est = est\n",
    "        self.switch = switch\n",
    "        self.val = val\n",
    "        self.xcols = xcols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        mask = X[:,self.switch]==self.val\n",
    "        self.est.fit(X[mask][:,self.xcols], y[mask])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        res = sp.lil_matrix((X.shape[0], 1), dtype=float)\n",
    "        mask = X[:,self.switch]==self.val\n",
    "        if hasattr(self.est, \"predict_proba\"):\n",
    "            res[mask,0] = self.est.predict_proba(X[mask][:,self.xcols])[:,[1]]\n",
    "        else:\n",
    "            estres = self.est.transform(X[mask][:,self.xcols])\n",
    "            res = sp.lil_matrix((X.shape[0], estres.shape[1]), dtype=float)\n",
    "            res[mask] = estres\n",
    "        return res.tocsr()\n",
    "\n",
    "class MegaSupervisedDecision(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, est, switch, xcols, n_jobs=-1):\n",
    "        super(MegaSupervisedDecision, self).__init__()\n",
    "        self.est = est\n",
    "        self.switch = switch\n",
    "        self.xcols = xcols\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.uniqs_ = np.unique(X[:,self.switch].ravel())\n",
    "        print self.uniqs_\n",
    "        self.ests_ = {\n",
    "            u: est for u, est in itertools.izip(self.uniqs_, Parallel(self.n_jobs)(\n",
    "                    (\n",
    "                        delayed(_estimator_fit)(\n",
    "                            clone(self.est),\n",
    "                            X[X[:,self.switch]==u][:,self.xcols],\n",
    "                            y[X[:,self.switch]==u])\n",
    "                        for u in self.uniqs_\n",
    "                    )\n",
    "                ))\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        xnew = np.zeros(X.shape[0], dtype=float)\n",
    "        for u, proba in itertools.izip(self.uniqs_, Parallel(self.n_jobs)(\n",
    "                (\n",
    "                    delayed(_estimator_predict_proba)(\n",
    "                        self.ests_[u],\n",
    "                        X[X[:,self.switch]==u][:,self.xcols])\n",
    "                    for u in self.uniqs_\n",
    "                )\n",
    "            )):\n",
    "            test = X[:,self.switch] == u\n",
    "            xnew[test] = proba\n",
    "        print xnew\n",
    "        return xnew.reshape(-1,1)\n",
    "\n",
    "class EqNonEqBinarizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, binarize=True):\n",
    "        super(EqNonEqBinarizer, self).__init__()\n",
    "        self.binarize = binarize\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.binarize:\n",
    "            self.est = LabelBinarizer(sparse_output=True)\n",
    "            self.est.fit(X[:,:2].astype(float).ravel())\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        xnew = np.where(np.array([X[:,0]==X[:,1],X[:,0]!=X[:,1],X[:,0]!=X[:,1]]), np.array([X[:,0],X[:,0],X[:,1]]), 0).T.astype(float)\n",
    "        if self.binarize:\n",
    "            return sp.hstack((\n",
    "                    self.est.transform(xnew[:,0].ravel()),\n",
    "                    self.est.transform(xnew[:,1].ravel()) + self.est.transform(xnew[:,2].ravel()),\n",
    "                ))\n",
    "        else:\n",
    "            return xnew\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        ene = EqNonEqBinarizer()\n",
    "        assert (ene.fit_transform(np.array(\n",
    "                [\n",
    "                    [1,1],\n",
    "                    [2,1],\n",
    "                    [1,2],\n",
    "                    [3,2],\n",
    "                    [3,3],\n",
    "                ])).toarray() == np.array([\n",
    "                [\n",
    "                    [1,0,0,0,0,0],\n",
    "                    [0,0,0,1,1,0],\n",
    "                    [0,0,0,1,1,0],\n",
    "                    [0,0,0,0,1,1],\n",
    "                    [0,0,1,0,0,0],\n",
    "                ]\n",
    "            ])).all()\n",
    "        del ene\n",
    "EqNonEqBinarizer.test()\n",
    "\n",
    "\n",
    "def block_calc_distances(X, metrics):\n",
    "    sparseok = distance_metrics().keys()\n",
    "    sparsemetrics = [_ for _ in metrics if _ in sparseok]\n",
    "    densemetrics = [_ for _ in metrics if _ not in sparseok]\n",
    "    \n",
    "    length = X.shape[-1]/2\n",
    "    Xs = []\n",
    "    for row in iter(X):\n",
    "        if len(densemetrics):\n",
    "            rowdense = row.todense()\n",
    "        Xs.append(np.array(\n",
    "            [pairwise_distances(row[:, :length], row[:, length:], metric) for metric in sparsemetrics] +\n",
    "            [pairwise_distances(rowdense[:, :length], rowdense[:, length:], metric) for metric in densemetrics]\n",
    "        ).ravel())\n",
    "    return np.array(Xs)\n",
    "\n",
    "class DistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, metrics, n_jobs=-1):\n",
    "        super(DistanceTransformer, self).__init__()\n",
    "        self.metrics = metrics\n",
    "        if n_jobs == -1:\n",
    "            n_jobs = cpu_count()\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        size = X.shape[0]\n",
    "        chsz, extra = divmod(size, self.n_jobs)\n",
    "        if extra:\n",
    "            chsz += 1\n",
    "        if size <= self.n_jobs:\n",
    "            self.n_jobs = size\n",
    "            chsz = 1\n",
    "        return np.vstack(\n",
    "            Parallel(self.n_jobs)(delayed(block_calc_distances)(X[i:min(i+chsz, size)], self.metrics) for i in xrange(0, size, chsz))\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        dist = Pipeline([\n",
    "                    ('transform', SwitchTransformer(est=TfidfVectorizer(), preaction=lambda X: X.ravel())),\n",
    "                    ('dists', DistanceTransformer(metrics=[\"cosine\"], n_jobs=1)),\n",
    "                ])\n",
    "        assert (np.round(dist.fit_transform(np.array([\n",
    "                [\"nokia 3310\", \"nokia 3310\"],\n",
    "                [\"samsung A5\", \"samsung A7\"],\n",
    "                [\"audi a4\", \"bmw a4\"],\n",
    "                [\"lada priora\", \"niva chevrolet\"],\n",
    "            ])), 2) == np.array([\n",
    "                [\n",
    "                        [0.0],\n",
    "                        [0.59],\n",
    "                        [0.59],\n",
    "                        [1.0],\n",
    "                ]\n",
    "            ])).all()\n",
    "        del dist\n",
    "DistanceTransformer.test()\n",
    "\n",
    "\n",
    "class ConstImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, fillvalue):\n",
    "        super(ConstImputer, self).__init__()\n",
    "        self.fillvalue = fillvalue\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X[X.astype(str)=='nan'] = self.fillvalue\n",
    "        return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def test():\n",
    "        tr = ConstImputer(\"fillvalue\")\n",
    "        assert (tr.fit_transform(np.array([\n",
    "                [\"nokia 3310\"],\n",
    "                [np.nan],\n",
    "                [\"lada priora\"],\n",
    "            ]).astype(object)) == np.array([\n",
    "                [\n",
    "                    [\"nokia 3310\"],\n",
    "                    [\"fillvalue\"],\n",
    "                    [\"lada priora\"],\n",
    "                ]\n",
    "            ])).all()\n",
    "        del tr\n",
    "ConstImputer.test()\n",
    "\n",
    "\n",
    "class IdFreq(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=1):\n",
    "        super(IdFreq, self).__init__()\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        ids, counts = np.unique(X.ravel(), return_counts=True)\n",
    "        self.whitelist = ids[counts >= self.threshold]\n",
    "        self.newid = ids[counts < self.threshold][0] if len(ids[counts < self.threshold]) else None\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        ids = np.unique(X.ravel())\n",
    "        for i in set(ids) ^ set(self.whitelist):\n",
    "            X[X==i] = self.newid\n",
    "        return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def test():\n",
    "        tr = IdFreq(2)\n",
    "        assert (tr.fit_transform(np.array([\n",
    "                [1],\n",
    "                [2],\n",
    "                [1],\n",
    "                [3],\n",
    "            ])) == np.array([\n",
    "                [\n",
    "                    [1],\n",
    "                    [2],\n",
    "                    [1],\n",
    "                    [2],\n",
    "                ]\n",
    "            ])).all()\n",
    "IdFreq.test()\n",
    "\n",
    "class SumClassifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.sum(axis=1)\n",
    "\n",
    "class CoordDist(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.sqrt((X[:,0] - X[:,1])**2 + (X[:,2] - X[:,3])**2).reshape(-1, 1)\n",
    "\n",
    "class LocationDist(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, thresholds=None):\n",
    "        self.thresholds = thresholds\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        xnew = np.array([X[:,[0,1]].ravel(), X[:,[2,3]].ravel(), X[:,[4,5]].ravel()]).T\n",
    "        loclist = np.unique(xnew[:,0])\n",
    "        self.locs_ = {}\n",
    "        for lid in loclist:\n",
    "            mask = xnew[:,0] == lid\n",
    "            self.locs_[lid] = xnew[mask,1:].mean(axis=0)\n",
    "            pass\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        xflat = np.array([X[:,[0,1]].ravel(), X[:,[2,3]].ravel(), X[:,[4,5]].ravel()]).T\n",
    "        xnew = np.zeros((xflat.shape[0], 1), dtype=float)\n",
    "        locs = set(np.unique(xflat[:,0]))\n",
    "        for lid in locs ^ set(self.locs_.keys()):\n",
    "            mask = xflat[:,0] == lid\n",
    "            xnew[mask,0] = np.nan\n",
    "        for lid in self.locs_.keys():\n",
    "            mask = xflat[:,0] == lid\n",
    "            xnew[mask,0] = np.sqrt(((xflat[mask,1:]-self.locs_[lid])**2).sum(axis=1))\n",
    "\n",
    "        result = sp.lil_matrix((xnew.shape[0], 2+len(self.thresholds)), dtype=int)\n",
    "        \n",
    "        mask = np.where(~np.isfinite(xnew))[0]\n",
    "        result[mask, np.array([0]).repeat(mask.shape[0])] = 1\n",
    "        \n",
    "        mask = np.where(xnew<self.thresholds[0])[0]\n",
    "        result[mask, np.array([1]).repeat(mask.shape[0])] = 1\n",
    "        for i in xrange(len(self.thresholds)-1):\n",
    "            mask = np.where(np.logical_and(xnew >= self.thresholds[i], xnew < self.thresholds[i+1]))[0]\n",
    "            result[mask, np.array([i+3]).repeat(mask.shape[0])] = 1\n",
    "        \n",
    "        mask = np.where(xnew>self.thresholds[-1])[0]\n",
    "        result[mask, np.array([2]).repeat(mask.shape[0])] = 1\n",
    "        \n",
    "        result = result[::2] + result[1::2]\n",
    "        return result.tocsr()\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        tr = LocationDist([10, 13, 15])\n",
    "        assert (tr.fit_transform(np.array([\n",
    "                        [1, 2, 10, 10, 20, 20],\n",
    "                        [2, 3, 10, 20, 20, 20],\n",
    "                        [3, 1, 0, 0, 0, 0]\n",
    "            ])) == np.array([\n",
    "                    [0, 1, 0, 1, 0],\n",
    "                    [0, 1, 0, 0, 1],\n",
    "                    [0, 0, 0, 1, 1],\n",
    "            ])).all()\n",
    "        del tr\n",
    "LocationDist.test()\n",
    "\n",
    "\n",
    "def json2dict(X):\n",
    "    def _inner(x):\n",
    "        res = {}\n",
    "        for k, v in json.loads(x).iteritems():\n",
    "            if isinstance(v, dict):\n",
    "                res[k] = v\n",
    "            else:\n",
    "                res[k+\"__\"] = \"1\"\n",
    "                res[k+\"___\"+v] = \"1\"\n",
    "        return res\n",
    "    return np.frompyfunc(_inner, 1, 1)(X)\n",
    "\n",
    "class JsonToDict(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_jobs=-1):\n",
    "        self.n_jobs = n_jobs\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if self.n_jobs == -1:\n",
    "            self.n_jobs = cpu_count()\n",
    "        size = X.shape[0]\n",
    "        chsz, extra = divmod(size, self.n_jobs)\n",
    "        if extra:\n",
    "            chsz += 1\n",
    "        if size <= self.n_jobs:\n",
    "            self.n_jobs = max(size, 1)\n",
    "            chsz = 1\n",
    "        return np.vstack(\n",
    "            Parallel(self.n_jobs)(delayed(json2dict)(X[i:min(i+chsz, size)]) for i in xrange(0, size, chsz))\n",
    "        )\n",
    "\n",
    "\n",
    "def jsonattrstrf_fit(X):\n",
    "    uniqs = np.array([])\n",
    "    iX = iter(X)\n",
    "    while True:\n",
    "        samples = np.array(list(itertools.islice(iX, 500))).ravel()\n",
    "        if samples.shape[0] == 0:\n",
    "            break\n",
    "        uniqs = np.unique(np.hstack(\n",
    "                (\n",
    "                    uniqs,\n",
    "                    np.array([\n",
    "                        k\n",
    "                        for x in samples\n",
    "                        for k, v in x.iteritems()\n",
    "                    ])\n",
    "                )\n",
    "            ))\n",
    "    return uniqs\n",
    "    return np.unique(np.array([\n",
    "                k\n",
    "                for x in X.ravel()\n",
    "                for k, v in x.iteritems()\n",
    "            ]))\n",
    "    def _inner(x):\n",
    "        return np.array([k for k, v in x.iteritems()])\n",
    "    return np.unique(np.hstack(np.frompyfunc(_inner, 1, 1)(X)))\n",
    "\n",
    "def jsonattrstrf_transform(X, keylist):\n",
    "    a = CountVectorizer().build_analyzer()\n",
    "    keys = {k: i for i, k in enumerate(keylist)}\n",
    "    keyslen = len(keys)\n",
    "    res = sp.lil_matrix((X.shape[0], 2*keyslen), dtype=float)\n",
    "    for i in xrange(X.shape[0]):\n",
    "        for k in set(X[i,0].keys()) | set(X[i,1].keys()):\n",
    "            if k not in keys:\n",
    "                continue\n",
    "            ki = keys[k]\n",
    "            if k not in X[i,0] or k not in X[i,1]:\n",
    "                res[i,keyslen + ki] = 1.0\n",
    "            else:\n",
    "                v1 = X[i,0][k]\n",
    "                v2 = X[i,1][k]\n",
    "                if not isinstance(v1, dict) and not isinstance(v2, dict):\n",
    "                    res[i,ki] = 1.0\n",
    "                elif isinstance(v1, dict) and isinstance(v2, dict):\n",
    "                    score = np.array([]).astype(float)\n",
    "                    for k1 in set(v1.keys()) | set(v2.keys()):\n",
    "                        if k1 not in v1 or k1 not in v2:\n",
    "                            score = np.hstack((score, np.array([0.0])))\n",
    "                        else:\n",
    "                            # jaccard\n",
    "                            j1 = set(a(v1[k1]))\n",
    "                            j2 = set(a(v2[k1]))\n",
    "                            if len(j1 | j2) == 0:\n",
    "                                score = np.hstack((score, np.array([1.0])))\n",
    "                            else:\n",
    "                                score = np.hstack((score, np.array([1.0 * len(j1 & j2) / len(j1 | j2)])))\n",
    "                    res[i,ki] = score.mean()\n",
    "                else:\n",
    "                    res[i,keyslen + ki] = 1.0\n",
    "    return res.tocsr()\n",
    "\n",
    "class JsonAttrsTrf(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_jobs=-1):\n",
    "        self.n_jobs = n_jobs\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.n_jobs == -1:\n",
    "            self.n_jobs = cpu_count()\n",
    "        size = X.shape[0]\n",
    "        chsz, extra = divmod(size, self.n_jobs)\n",
    "        if extra:\n",
    "            chsz += 1\n",
    "        if size <= self.n_jobs:\n",
    "            self.n_jobs = max(size, 1)\n",
    "            chsz = 1\n",
    "        keys = np.unique(np.hstack(\n",
    "            Parallel(self.n_jobs)(delayed(jsonattrstrf_fit)(X[i:min(i+chsz, size)]) for i in xrange(0, size, chsz))\n",
    "        ))\n",
    "        self.keys_ = keys\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        size = X.shape[0]\n",
    "        chsz, extra = divmod(size, self.n_jobs)\n",
    "        if extra:\n",
    "            chsz += 1\n",
    "        if size <= self.n_jobs:\n",
    "            self.n_jobs = max(size, 1)\n",
    "            chsz = 1\n",
    "        result = sp.vstack(\n",
    "            Parallel(self.n_jobs)(delayed(jsonattrstrf_transform)(X[i:min(i+chsz, size)], self.keys_) for i in xrange(0, size, chsz))\n",
    "        )\n",
    "        print result.shape\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        tr = JsonAttrsTrf(n_jobs=1)\n",
    "        assert (np.round(tr.fit_transform(np.array([\n",
    "                    [{\"something\": 1}, {\"something\": 1}],\n",
    "                    [{\"something\": 1}, {\"otherthing\": 1}],\n",
    "                    [{\"somedict\": {\"0\": \"my car\", \"1\": \"2016-01-02\"}}, {\"somedict\": {\"0\": \"your car\", \"1\": \"2016-01-02\"}}],\n",
    "            ])).toarray(), 2) == np.array([\n",
    "                    [ 0., 0.,   1., 0., 0., 0. ],\n",
    "                    [ 0., 0.,   0., 1., 0., 1. ],\n",
    "                    [ 0., 0.67, 0., 0., 0., 0. ],\n",
    "            ])).all()\n",
    "        del tr\n",
    "JsonAttrsTrf.test()\n",
    "\n",
    "\n",
    "class IdRecoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        super(IdRecoder, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        uniqs = sorted(itertools.izip(*np.unique(X.ravel(), return_counts=True)), key=lambda x: x[1], reverse=True)\n",
    "        self.uniq_ = uniqs\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        res = np.zeros(X.shape, dtype=float)\n",
    "        res[:] = np.nan\n",
    "        for i, (uid, cnt) in enumerate(self.uniq_):\n",
    "            res[X==uid] = i\n",
    "        res[~np.isfinite(res)] = len(self.uniq_)\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def test():\n",
    "        tr = IdRecoder()\n",
    "        assert (tr.fit_transform(np.array([\n",
    "                    [10],\n",
    "                    [11],\n",
    "                    [20],\n",
    "                    [10],\n",
    "                    [11],\n",
    "                    [11],\n",
    "                ])) == np.array([\n",
    "                    [1.],\n",
    "                    [0.],\n",
    "                    [2.],\n",
    "                    [1.],\n",
    "                    [0.],\n",
    "                    [0.],\n",
    "                ])).all()\n",
    "IdRecoder.test()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CatProba(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, only_sum=False):\n",
    "        self.only_sum = only_sum\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        ids = np.unique(X.ravel())\n",
    "        self.probabilities_ = {}\n",
    "        for cat in ids:\n",
    "            mask = (X==cat).any(axis=1)\n",
    "            self.probabilities_[cat] = 1.0 * y[mask].sum() / len(y[mask])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        result = sp.lil_matrix((X.shape[0], len(self.probabilities_.keys())+1), dtype=float)\n",
    "        for i, cat in enumerate(self.probabilities_.keys()):\n",
    "            mask = np.where((X==cat).any(axis=1))[0]\n",
    "            result[mask, np.array([i]).repeat(mask.shape[0])] = self.probabilities_[cat]\n",
    "        result[:,len(self.probabilities_.keys())] = result.sum(axis=1)\n",
    "        if self.only_sum:\n",
    "            return result[:,-1].tocsr()\n",
    "        else:\n",
    "            return result.tocsr()\n",
    "    \n",
    "#     def fit_transform(self, X, y=None):\n",
    "#         ids = np.unique(X.ravel())\n",
    "#         self.probabilities_ = {}\n",
    "#         result = sp.lil_matrix((X.shape[0], 0), dtype=float)\n",
    "#         for cat in ids:\n",
    "#             mask = np.where((X==cat).any(axis=1))[0]\n",
    "#             self.probabilities_[cat] = 1.0 * y[mask].sum() / len(y[mask])\n",
    "#             catres = sp.lil_matrix((X.shape[0], 1), dtype=float)\n",
    "#             catres[mask, np.array([0]).repeat(mask.shape[0])] = self.probabilities_[cat]\n",
    "#             result = sp.vstack([\n",
    "#                     result,\n",
    "#                     catres\n",
    "#                 ])\n",
    "#         result = sp.vstack([\n",
    "#                 result,\n",
    "#                 result.sum(axis=1)\n",
    "#             ])\n",
    "#         return result\n",
    "        \n",
    "    @staticmethod\n",
    "    def test():\n",
    "        tr = CatProba()\n",
    "        assert (tr.fit_transform(np.array([\n",
    "                        [1, 1],\n",
    "                        [2, 2],\n",
    "                        [1, 1],\n",
    "                        [3, 3],\n",
    "            ]), np.array([\n",
    "                        [1],\n",
    "                        [1],\n",
    "                        [0],\n",
    "                        [0],\n",
    "                    ])).toarray() == np.array([\n",
    "                    [ 0.5,  0. ,  0. ,  0.5],\n",
    "                    [ 0. ,  1. ,  0. ,  1. ],\n",
    "                    [ 0.5,  0. ,  0. ,  0.5],\n",
    "                    [ 0. ,  0. ,  0. ,  0. ],\n",
    "                ])).all()\n",
    "        del tr\n",
    "CatProba.test()\n",
    "\n",
    "\n",
    "class MetaCategoryEstimator(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    catfield = \"categoryID\"\n",
    "    pcatfield = \"parentCategoryID\"\n",
    "    \n",
    "    def __init__(self, itrain, itest, category, parent, common):\n",
    "        super(MetaCategoryEstimator, self).__init__()\n",
    "        self.itrain = itrain\n",
    "        self.itest = itest\n",
    "        self.category = category\n",
    "        self.parent = parent\n",
    "        self.common = common\n",
    "        cats = pd.read_csv(\"data/Category.csv\", index_col=0)\n",
    "        self.pcat_by_cat_ = {k: v for k, v in cats[\"parentCategoryID\"].iteritems()}\n",
    "        self.cats_by_pcat_ = {}\n",
    "        for k, v in self.pcat_by_cat_.iteritems():\n",
    "            self.cats_by_pcat_[v] = self.cats_by_pcat_.get(v, []) + [k]\n",
    "        pass\n",
    "    \n",
    "    def parent_fit(self, X, y=None):\n",
    "        from sklearn.cross_validation import train_test_split\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        \n",
    "        cats = pd.read_csv(\"data/ItemInfo_train.csv_%s.csv\" % self.__class__.catfield, index_col=0)\n",
    "        pcats = pd.read_csv(\"data/ItemInfo_train.csv_%s.csv\" % self.__class__.pcatfield, index_col=0)\n",
    "        self.pcats_ = list(np.unique(pcats[self.__class__.pcatfield].values.ravel()))\n",
    "        pr = pd.read_csv(\"data/ItemPairs_train.csv\").loc[self.itrain]\n",
    "        pr = pd.merge(pr, cats, left_on=\"itemID_1\", right_index=True, how=\"inner\", sort=False)\n",
    "        pr = pd.merge(pr, pcats, left_on=\"itemID_1\", right_index=True, how=\"inner\", sort=False)\n",
    "        del cats\n",
    "        del pcats\n",
    "        \n",
    "        self.pcat_itrain_ = []\n",
    "        self.pcat_itest_ = []\n",
    "        for pcatid in self.pcats_:\n",
    "            catids = self.cats_by_pcat_[pcatid]\n",
    "            pcatindices = pd.Index(np.hstack([\n",
    "                        cat_itrain\n",
    "                        for catid, cat_itrain in itertools.izip(self.cats_, self.cat_itrain_)\n",
    "                        if catid in catids\n",
    "                    ]))\n",
    "            itrain, itest = train_test_split(pcatindices, test_size=0.05, random_state=42, stratify=pr.loc[pcatindices, \"isDuplicate\"].values)\n",
    "            self.pcat_itrain_.append(itrain)\n",
    "            self.pcat_itest_.append(itest)\n",
    "\n",
    "        for pcatid, pcatindices, itest in itertools.izip(self.pcats_, self.pcat_itrain_, self.pcat_itest_):\n",
    "            est = None\n",
    "            transformers_list = []\n",
    "            for columns, transformer in self.parent:\n",
    "                if columns == []:\n",
    "                    est = clone(transformer)\n",
    "                    break\n",
    "                transformers_list.append((columns, transformer))\n",
    "                x = _fit_transform(\"train\", pcatindices, columns, transformer)\n",
    "                del x\n",
    "                _transform(\"train\", pcatindices, itest, columns, transformer)\n",
    "            if est is not None:\n",
    "                Xs = [_fit_transform(\"train\", pcatindices, columns, transformer) for columns, transformer in transformers_list]\n",
    "                if any(sp.issparse(f) for f in Xs):\n",
    "                    Xs = sp.hstack([_ for _ in Xs if _.shape[-1]]).tocsr()\n",
    "                else:\n",
    "                    Xs = np.hstack([_ for _ in Xs if _.shape[-1]])\n",
    "                print \"PCATID:\", pcatid, Xs.shape\n",
    "                est.fit(Xs, pr.loc[pcatindices, \"isDuplicate\"].values)\n",
    "                del Xs\n",
    "                \n",
    "                Xs = [_transform(\"train\", pcatindices, itest, columns, transformer) for columns, transformer in transformers_list]\n",
    "                if any(sp.issparse(f) for f in Xs):\n",
    "                    Xs = sp.hstack([_ for _ in Xs if _.shape[-1]]).tocsr()\n",
    "                else:\n",
    "                    Xs = np.hstack([_ for _ in Xs if _.shape[-1]])\n",
    "                print \"SCORE:\", roc_auc_score(pr.loc[itest, \"isDuplicate\"].values, est.predict_proba(Xs)[:,1])\n",
    "                print classification_report(pr.loc[itest, \"isDuplicate\"].values, est.predict(Xs))\n",
    "                del Xs\n",
    "                dump(est, os.getenv(\"CACHEDIR\", \"data/avito-cache/meta-pcat-%d.est\" % pcatid))\n",
    "                del est\n",
    "    \n",
    "    def parent_predict_proba(self, mode, catid, indices):\n",
    "        pdparams = {}\n",
    "        if mode == \"test\":\n",
    "            pdparams[\"index_col\"] = 0\n",
    "        pcats = pd.read_csv(\"data/ItemInfo_%s.csv_%s.csv\" % (mode, self.__class__.pcatfield), index_col=0)\n",
    "        pr = pd.read_csv(\"data/ItemPairs_%s.csv\" % mode, **pdparams).loc[indices]\n",
    "        pr = pd.merge(pr, pcats, left_on=\"itemID_1\", right_index=True, how=\"inner\", sort=False)\n",
    "        del pcats\n",
    "        \n",
    "        result = pd.DataFrame([], index=indices)\n",
    "        result[\"proba\"] = 0.0\n",
    "        result[\"pred\"] = 0.0\n",
    "        \n",
    "        pcatid = self.pcat_by_cat_[catid]\n",
    "        pcat_itrain = self.pcat_itrain_[self.pcats_.index(pcatid)]\n",
    "        \n",
    "        print indices\n",
    "        est = None\n",
    "        transformers_list = []\n",
    "        for columns, transformer in self.parent:\n",
    "            if columns == []:\n",
    "                est = clone(transformer)\n",
    "                break\n",
    "            transformers_list.append((columns, transformer))\n",
    "        if est is not None:\n",
    "            Xs = [_transform(mode, pcat_itrain, indices, columns, transformer) for columns, transformer in transformers_list]\n",
    "            if any(sp.issparse(f) for f in Xs):\n",
    "                Xs = sp.hstack([_ for _ in Xs if _.shape[-1]]).tocsr()\n",
    "            else:\n",
    "                Xs = np.hstack([_ for _ in Xs if _.shape[-1]])\n",
    "            print \"PCATID:\", pcatid, Xs.shape\n",
    "            est = load(os.getenv(\"CACHEDIR\", \"data/avito-cache/meta-pcat-%d.est\" % pcatid))\n",
    "            result.loc[indices, \"proba\"] = est.predict_proba(Xs)[:,1]\n",
    "            result.loc[indices, \"pred\"] = est.predict(Xs)\n",
    "            del Xs\n",
    "            del est\n",
    "\n",
    "        result[\"proba0\"] = 1.0 - result[\"proba\"]\n",
    "        print result\n",
    "        return result[[\"proba0\", \"proba\"]].values\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        from sklearn.cross_validation import train_test_split\n",
    "        from sklearn.metrics import classification_report\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        \n",
    "        assert (X.ravel() == self.itrain.values).all()\n",
    "        \n",
    "        cats = pd.read_csv(\"data/ItemInfo_train.csv_%s.csv\" % self.__class__.catfield, index_col=0)\n",
    "        self.cats_ = list(np.unique(cats[self.__class__.catfield].values.ravel()))\n",
    "        pr = pd.read_csv(\"data/ItemPairs_train.csv\").loc[self.itrain]\n",
    "        pr = pd.merge(pr, cats, left_on=\"itemID_1\", right_index=True, how=\"inner\", sort=False)\n",
    "        del cats\n",
    "        \n",
    "        self.cat_itrain_ = []\n",
    "        self.cat_itest_ = []\n",
    "        for catid in self.cats_:\n",
    "            catindices = pr[pr[self.__class__.catfield] == catid].index\n",
    "            itrain, itest = train_test_split(catindices, test_size=0.05, random_state=42, stratify=pr.loc[catindices, \"isDuplicate\"].values)\n",
    "            self.cat_itrain_.append(itrain)\n",
    "            self.cat_itest_.append(itest)\n",
    "\n",
    "#         self.common_fit(X, y)\n",
    "        self.parent_fit(X, y)\n",
    "\n",
    "        for catid, catindices, itest in itertools.izip(self.cats_, self.cat_itrain_, self.cat_itest_):\n",
    "            est = None\n",
    "            transformers_list = []\n",
    "            for columns, transformer in self.category:\n",
    "                if columns == []:\n",
    "                    est = clone(transformer)\n",
    "                    break\n",
    "                transformers_list.append((columns, transformer))\n",
    "                x = _fit_transform(\"train\", catindices, columns, transformer)\n",
    "                del x\n",
    "                _transform(\"train\", catindices, itest, columns, transformer)\n",
    "            if est is not None:\n",
    "                Xs = [_fit_transform(\"train\", catindices, columns, transformer) for columns, transformer in transformers_list]\n",
    "                Xs += [self.parent_predict_proba(\"train\", catid, catindices)]\n",
    "                if any(sp.issparse(f) for f in Xs):\n",
    "                    Xs = sp.hstack([_ for _ in Xs if _.shape[-1]]).tocsr()\n",
    "                else:\n",
    "                    Xs = np.hstack([_ for _ in Xs if _.shape[-1]])\n",
    "                print \"CATID:\", catid, Xs.shape\n",
    "                est.fit(Xs, pr.loc[catindices, \"isDuplicate\"].values)\n",
    "                del Xs\n",
    "                \n",
    "                Xs = [_transform(\"train\", catindices, itest, columns, transformer) for columns, transformer in transformers_list]\n",
    "                Xs += [self.parent_predict_proba(\"train\", catid, itest)]\n",
    "                if any(sp.issparse(f) for f in Xs):\n",
    "                    Xs = sp.hstack([_ for _ in Xs if _.shape[-1]]).tocsr()\n",
    "                else:\n",
    "                    Xs = np.hstack([_ for _ in Xs if _.shape[-1]])\n",
    "                print \"SCORE:\", roc_auc_score(pr.loc[itest, \"isDuplicate\"].values, est.predict_proba(Xs)[:,1])\n",
    "                print classification_report(pr.loc[itest, \"isDuplicate\"].values, est.predict(Xs))\n",
    "                del Xs\n",
    "                dump(est, os.getenv(\"CACHEDIR\", \"data/avito-cache/meta-cat-%d.est\" % catid))\n",
    "                del est\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        cats = pd.read_csv(\"data/ItemInfo_test.csv_%s.csv\" % self.__class__.catfield, index_col=0)\n",
    "        pr = pd.read_csv(\"data/ItemPairs_test.csv\", index_col=0).loc[self.itest]\n",
    "        pr = pd.merge(pr, cats, left_on=\"itemID_1\", right_index=True, how=\"inner\", sort=False)\n",
    "        del cats\n",
    "        \n",
    "        result = pd.DataFrame([], index=self.itest)\n",
    "        result[\"proba\"] = 0.0\n",
    "        result[\"pred\"] = 0.0\n",
    "        for catid, cat_itrain in itertools.izip(self.cats_, self.cat_itrain_):\n",
    "            catindices = pr[pr[self.__class__.catfield] == catid].index\n",
    "            print catindices\n",
    "            est = None\n",
    "            transformers_list = []\n",
    "            for columns, transformer in self.category:\n",
    "                if columns == []:\n",
    "                    est = clone(transformer)\n",
    "                    break\n",
    "                transformers_list.append((columns, transformer))\n",
    "            if est is not None:\n",
    "                Xs = [_transform(\"test\", cat_itrain, catindices, columns, transformer) for columns, transformer in transformers_list]\n",
    "                Xs += [self.parent_predict_proba(\"test\", catid, catindices)]\n",
    "                if any(sp.issparse(f) for f in Xs):\n",
    "                    Xs = sp.hstack([_ for _ in Xs if _.shape[-1]]).tocsr()\n",
    "                else:\n",
    "                    Xs = np.hstack([_ for _ in Xs if _.shape[-1]])\n",
    "                print \"CATID:\", catid, Xs.shape\n",
    "                est = load(os.getenv(\"CACHEDIR\", \"data/avito-cache/meta-cat-%d.est\" % catid))\n",
    "                result.loc[catindices, \"proba\"] = est.predict_proba(Xs)[:,1]\n",
    "                result.loc[catindices, \"pred\"] = est.predict(Xs)\n",
    "                del Xs\n",
    "                del est\n",
    "        result[\"proba0\"] = 1.0 - result[\"proba\"]\n",
    "        print result\n",
    "        return result[[\"proba0\", \"proba\"]].values\n",
    "\n",
    "class TitleDist(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, analyzer=None):\n",
    "        super(TitleDist, self).__init__()\n",
    "        if analyzer is None:\n",
    "            analyzer = CountVectorizer()\n",
    "        self.analyzer = analyzer\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        a = self.analyzer.build_analyzer()\n",
    "        def _inner(x, y):\n",
    "            setx = set(a(x))\n",
    "            sety = set(a(y))\n",
    "            if len(setx | sety) == 0:\n",
    "                return 0.0, 0.0\n",
    "            return 1.0 * len(setx & sety) / len(setx | sety), 1.0 * len(setx ^ sety) / len(setx | sety)\n",
    "        return np.vstack(np.frompyfunc(_inner, 2, 2)(X[:,0], X[:,1])).T\n",
    "\n",
    "\n",
    "def _title_edit_dist_transform(x, y):\n",
    "    return [\n",
    "        editdistance.eval(unicode(x, \"UTF-8\"), unicode(y, \"UTF-8\")),\n",
    "        dld(unicode(x, \"UTF-8\"), unicode(y, \"UTF-8\"))\n",
    "    ]\n",
    "class TitleEditDist(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_jobs=-1):\n",
    "        super(TitleEditDist, self).__init__()\n",
    "        self.n_jobs = n_jobs\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array(\n",
    "            Parallel(self.n_jobs)(\n",
    "                delayed(_title_edit_dist_transform)(x, y) for x, y in iter(X)\n",
    "            )\n",
    "        )\n",
    "\n",
    "def _token_dist_transform(x, y):\n",
    "    ax = set(analyzer(unicode(x, \"UTF-8\")))\n",
    "    ay = set(analyzer(unicode(y, \"UTF-8\")))\n",
    "    sx = ''.join(ax)\n",
    "    sy = ''.join(ay)\n",
    "    l = len(ax) + len(ay)\n",
    "    if l==0:\n",
    "        return 0.0\n",
    "    c = 0\n",
    "    for t in ax:\n",
    "        if t in sy:\n",
    "            c+=1\n",
    "    for t in ay:\n",
    "        if t in sx:\n",
    "            c+=1\n",
    "    return 1.0 * c / l\n",
    "class TokenDist(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_jobs=-1):\n",
    "        super(TokenDist, self).__init__()\n",
    "        self.n_jobs = n_jobs\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array(\n",
    "            Parallel(self.n_jobs)(\n",
    "                delayed(_token_dist_transform)(x, y) for x, y in iter(X)\n",
    "            )\n",
    "        ).reshape(-1,1)\n",
    "    \n",
    "\n",
    "class ConstantCol(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        super(ConstantCol, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if sp.issparse(X):\n",
    "            if X.shape[-1] == 0:\n",
    "                return sp.csr_matrix(np.array([1.0]).repeat(X.shape[0]).reshape(-1,1))\n",
    "            return sp.hstack((X, np.array([1]).repeat(X.shape[0]).reshape(-1,1))).tocsr()\n",
    "        else:\n",
    "            return np.hstack((X, np.array([1]).repeat(X.shape[0]).reshape(-1,1)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def test():\n",
    "        tr = ConstantCol()\n",
    "        print tr.fit_transform(np.array([[1,2],[3,4],[5,6]]))\n",
    "        print tr.fit_transform(np.array([[],[],[]]))\n",
    "        print tr.fit_transform(sp.csr_matrix(np.array([[1,2],[3,4],[5,6]]))).toarray()\n",
    "        print tr.fit_transform(sp.csr_matrix(np.array([[],[],[]]))).toarray()\n",
    "ConstantCol.test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _attrsJSON_dist(X):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    a = CountVectorizer().build_analyzer()\n",
    "    def _inner(x, y):\n",
    "        try:\n",
    "            scores = []\n",
    "            x = dict(\n",
    "                (\"___\".join([k, v]), 1) if not isinstance(v, dict) else (k+\"__\", v)\n",
    "                for k, v in json.loads(x).iteritems()\n",
    "            )\n",
    "            y = dict(\n",
    "                (\"___\".join([k, v]), 1) if not isinstance(v, dict) else (k+\"__\", v)\n",
    "                for k, v in json.loads(y).iteritems()\n",
    "            )\n",
    "            kx = set(x.keys())\n",
    "            ky = set(y.keys())\n",
    "            l = len(kx | ky)\n",
    "            if l == 0:\n",
    "                return 1.0\n",
    "            for k in kx|ky:\n",
    "                if all((k in x, k in y)):\n",
    "                    if all((isinstance(x[k], int), isinstance(y[k], int))):\n",
    "                        scores.append(1.0)\n",
    "                    elif any((isinstance(x[k], int), isinstance(y[k], int))):\n",
    "                        scores.append(1.0)\n",
    "                    else:\n",
    "                        dscore = []\n",
    "                        for k1 in set(x[k].keys()) | set(y[k].keys()):\n",
    "                            if any((k1 not in x[k], k1 not in y[k])):\n",
    "                                dscore.append(0.0)\n",
    "                            else:\n",
    "                                # jaccard\n",
    "                                j1 = set(a(x[k][k1]))\n",
    "                                j2 = set(a(x[k][k1]))\n",
    "                                if len(j1 | j2) == 0:\n",
    "                                    dscore.append(1.0)\n",
    "                                else:\n",
    "                                    dscore.append(1.0 * len(j1&j2) / len(j1|j2))\n",
    "                        scores.append(np.array(dscore).mean())\n",
    "                else:\n",
    "                    scores.append(0.0)\n",
    "            if len(scores) == 0:\n",
    "                return 1.0\n",
    "            return 1.0 * np.array(scores).mean()\n",
    "        except:\n",
    "            import sys\n",
    "            sys.excepthook(*sys.exc_info())\n",
    "            return np.nan\n",
    "    return np.frompyfunc(_inner, 2, 1)(X[:,0], X[:,1]).astype(float).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class IdPairsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        super(IdPairsTransformer, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.digits_ = np.max(np.ceil(np.log10(np.nan_to_num(X.ravel()) + 1))) + 1\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        xnew = np.zeros((X.shape[0], 2), dtype=X.dtype)\n",
    "        xnew[:,0] = X[:,0] * 10**self.digits_ + X[:,1]\n",
    "        xnew[:,1] = X[:,1] * 10**self.digits_ + X[:,0]\n",
    "        return np.where(X[:,0] >= X[:,1], xnew[:,0], xnew[:,1]).reshape(-1,1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def test():\n",
    "        tr = Pipeline([\n",
    "                ('tr1', IdPairsTransformer()),\n",
    "                ('tr2', IdRecoder()),\n",
    "            ])\n",
    "        assert (tr.fit_transform(np.array([\n",
    "                    [634050, 634050],\n",
    "                    [634010, 634050],\n",
    "                    [634050, 634010],\n",
    "                    [634000, 634000],\n",
    "                ])) == np.array([\n",
    "                    [2.],\n",
    "                    [0.],\n",
    "                    [0.],\n",
    "                    [1.],\n",
    "                ])).all()\n",
    "IdPairsTransformer.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class PriceTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        super(PriceTransformer, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder_ = LabelBinarizer()\n",
    "        self.encoder_.fit(\n",
    "            np.nan_to_num(\n",
    "                np.round(\n",
    "                    np.log10(X+1) - 0.2\n",
    "                ).ravel()))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        res = self.encoder_.transform(\n",
    "            np.nan_to_num(\n",
    "                np.round(\n",
    "                    np.log10(X+1) - 0.2\n",
    "                ).ravel())\n",
    "        )\n",
    "        xnew = np.zeros(res.shape[0], dtype=float)\n",
    "        for i, n in enumerate(range(res.shape[1]), 1):\n",
    "            xnew[res[:,n]==1] = i\n",
    "        return xnew.reshape(X.shape)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.encoder_ = LabelBinarizer()\n",
    "        res = self.encoder_.fit_transform(\n",
    "            np.nan_to_num(\n",
    "                np.round(\n",
    "                    np.log10(X+1) - 0.2\n",
    "                ).ravel()))\n",
    "        xnew = np.zeros(res.shape[0], dtype=float)\n",
    "        for i, n in enumerate(range(res.shape[1]), 1):\n",
    "            xnew[res[:,n]==1] = i\n",
    "        return xnew.reshape(X.shape)\n",
    "    \n",
    "class PriceOutlierTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        super(PriceOutlierTransformer, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.cats_ = np.unique(X[:,0])\n",
    "        self.median_ = np.array([\n",
    "                np.median(np.nan_to_num(np.log(X[X[:,0]==cat][:,[1,2]].ravel())))\n",
    "                for cat in self.cats_\n",
    "            ])\n",
    "        self.std_ = np.array([\n",
    "                np.std(np.nan_to_num(np.log(X[X[:,0]==cat][:,[1,2]].ravel())))\n",
    "                for cat in self.cats_\n",
    "            ])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X[:,[1,2]] = np.nan_to_num(np.log(X[:,[1,2]]))\n",
    "        xnew = np.zeros((X.shape[0], 3), dtype=float)\n",
    "        for cat, median, std in itertools.izip(self.cats_, self.median_, self.std_):\n",
    "            mask = X[:,0] == cat\n",
    "            for m in range(xnew.shape[-1]):\n",
    "                xnew[mask,m] += np.sum((X[mask][:,[1,2]] > median + (m+1) * std) + (X[mask][:,[1,2]] < median - (m+1) * std), axis=1)\n",
    "        return xnew\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def CoordAnchor_get_dist(lats, lons, clat, clon):\n",
    "    return np.min(\n",
    "        np.hstack((\n",
    "                np.sqrt( (lats - clat)**2 + (lons - clon)**2 ).reshape(-1,1),\n",
    "                np.sqrt( (lats - clat)**2 + (-1 * lons - clon)**2 ).reshape(-1,1),\n",
    "            )), axis=1\n",
    "    )\n",
    "\n",
    "class CoordAnchor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_jobs=-1):\n",
    "        super(CoordAnchor, self).__init__()\n",
    "        self.n_jobs = n_jobs\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        xflat = np.hstack((\n",
    "            X[:,[0,1]].reshape(-1,1),\n",
    "            X[:,[2,3]].reshape(-1,1),\n",
    "            X[:,[4,5]].reshape(-1,1),\n",
    "        ))\n",
    "        self.uniqs_ = np.array(list(itertools.izip(\n",
    "                    *list(sorted(\n",
    "                        itertools.izip(\n",
    "                            *list(np.unique(xflat[:,0], return_counts=True))\n",
    "                        ), key=lambda x: x[1], reverse=True\n",
    "                    ))\n",
    "        ))[0])\n",
    "        self.centers_ = np.array([\n",
    "                (xflat[mask,1].mean(), xflat[mask,2].mean())\n",
    "                for mask in (xflat[:,0] == i for i in self.uniqs_)\n",
    "        ])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        xflat = np.hstack((\n",
    "            X[:,[0,1]].reshape(-1,1),\n",
    "            X[:,[2,3]].reshape(-1,1),\n",
    "            X[:,[4,5]].reshape(-1,1),\n",
    "        ))\n",
    "        xids = np.zeros(xflat.shape[0])\n",
    "        xcurdists = np.zeros(xflat.shape[0], dtype=float)\n",
    "\n",
    "#         for i, xcurdists in itertools.izip(\n",
    "#             self.uniqs_, Parallel(self.n_jobs)(\n",
    "#                 delayed(CoordAnchor_get_dist)(xflat[:,1], xflat[:,2], lat, lon)\n",
    "#                 for lat, lon in self.centers_\n",
    "#             )\n",
    "#         ):\n",
    "        for i, (lat, lon) in itertools.izip(self.uniqs_, self.centers_):\n",
    "            xcurdists = np.min(\n",
    "                np.hstack((\n",
    "                        np.sqrt( (xflat[:,1] - lat)**2 + (xflat[:,2] - lon)**2 ).reshape(-1,1),\n",
    "                        np.sqrt( (xflat[:,1] - lat)**2 + (-1 * xflat[:,2] - lon)**2 ).reshape(-1,1),\n",
    "                    )), axis=1\n",
    "            )\n",
    "            try:\n",
    "                mask = xcurdists < xdists\n",
    "            except NameError:\n",
    "                xdists = xcurdists\n",
    "                mask = xcurdists <= xdists\n",
    "            xids[mask] = i\n",
    "            xdists[mask] = xcurdists[mask]\n",
    "        return xids.reshape(-1, 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def test():\n",
    "        tr = CoordAnchor()\n",
    "        assert (tr.fit_transform(np.array([\n",
    "                    [1, 1, 29.0, 28.0, 31.0, 32.0],\n",
    "                    [1, 1, 31.0, 28.0, 28.0, 32.0],\n",
    "                    [2, 2, 61.0, 57.0, 62.0, 59.0],\n",
    "                    [2, 2, 59.0, 57.0, 62.0, 61.0],\n",
    "                    [1, 2, 59.0, 57.0, 62.0, 61.0],\n",
    "                ])) == np.array([\n",
    "                    [1, 1],\n",
    "                    [1, 1],\n",
    "                    [2, 2],\n",
    "                    [2, 2],\n",
    "                    [2, 2],\n",
    "                ])).all()\n",
    "CoordAnchor.test()\n",
    "\n",
    "class MyLabelBinarizer(LabelBinarizer):\n",
    "    def __init__(self, neg_label=0, pos_label=1, sparse_output=False):\n",
    "        super(MyLabelBinarizer, self).__init__(neg_label, pos_label, sparse_output)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return super(MyLabelBinarizer, self).fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class FreqTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        super(FreqTransformer, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        xnew = np.zeros(X.shape).ravel()\n",
    "        uniqs = {k: v for k, v in itertools.izip(*np.unique(X.ravel(), return_counts=True))}\n",
    "        xnew = np.frompyfunc(lambda x: uniqs[x], 1, 1)(X.ravel())\n",
    "        return xnew.reshape(X.shape).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def test():\n",
    "        tr = FreqTransformer()\n",
    "        assert (tr.fit_transform(np.array([\n",
    "                    [1,3],\n",
    "                    [5,2],\n",
    "                    [1,5],\n",
    "                    [3,5],\n",
    "                ])) == np.array([\n",
    "                    [2,2],\n",
    "                    [3,1],\n",
    "                    [2,3],\n",
    "                    [2,3],\n",
    "                ])).all()\n",
    "# FreqTransformer.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _pairs_dist_transform(X, metrics):\n",
    "    from sklearn.metrics.pairwise import pairwise_distances\n",
    "    l = X.shape[-1] / 2\n",
    "    return np.array([\n",
    "        [\n",
    "            pairwise_distances(X[i,range(0, l)].reshape(1,-1), X[i,range(l, l*2)].reshape(1,-1), metric).ravel()[0]\n",
    "            for metric in metrics\n",
    "        ]\n",
    "        for i in range(X.shape[0])\n",
    "    ])\n",
    "    \n",
    "class PairsDist(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, metrics, n_jobs=-1):\n",
    "        super(PairsDist, self).__init__()\n",
    "        self.metrics = metrics\n",
    "        self.n_jobs = n_jobs\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        n_jobs = self.n_jobs\n",
    "        if n_jobs == -1:\n",
    "            n_jobs = cpu_count()\n",
    "        size = X.shape[0]\n",
    "        chsz, extra = divmod(size, n_jobs)\n",
    "        if extra:\n",
    "            chsz += 1\n",
    "        if size <= n_jobs:\n",
    "            n_jobs = max(size, 1)\n",
    "            chsz = 1\n",
    "        return np.vstack(\n",
    "            Parallel(n_jobs)(\n",
    "                delayed(_pairs_dist_transform)(X[i:min(i+chsz, size)], self.metrics)\n",
    "                for i in xrange(0, size, chsz)\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_train_indices(column=\"categoryID\"):\n",
    "    import gzip\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.utils import shuffle\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    prtrain = pd.read_csv(\"data/ItemPairs_train.csv\")\n",
    "    if column == \"all\":\n",
    "        return prtrain.index\n",
    "    \n",
    "    prtest = pd.read_csv(\"data/ItemPairs_test.csv\", index_col=0)\n",
    "    prtest = pd.merge(prtest,\n",
    "                     pd.read_csv(\"data/ItemInfo_test.csv_%s.csv\" % column, index_col=0),\n",
    "                     left_on=\"itemID_1\", right_index=True, how=\"inner\", sort=False)\n",
    "    catdist = prtest[column].value_counts() / len(prtest)\n",
    "    del prtest\n",
    "    \n",
    "    prtrain = pd.merge(prtrain,\n",
    "                       pd.read_csv(\"data/ItemInfo_train.csv_%s.csv\" % column, index_col=0),\n",
    "                       left_on=\"itemID_1\", right_index=True, how=\"inner\", sort=False)\n",
    "    \n",
    "    indices = np.array([])\n",
    "    trainsize = len(prtrain)\n",
    "    for cat, dist in catdist.iteritems():\n",
    "        trcatdist = len(prtrain[prtrain[column] == cat])\n",
    "        if trcatdist < int(1.0 * dist * trainsize):\n",
    "            trainsize = int(1.0 * trainsize * trcatdist / (dist * trainsize))\n",
    "    for cat, dist in catdist.iteritems():\n",
    "        indices = np.hstack((indices, shuffle(prtrain[prtrain[column] == cat].index, random_state=1)[:int(dist*trainsize)]))\n",
    "    \n",
    "    indices = pd.Index(np.sort(indices.astype(int)))\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def _get_X(mode, indices, columns):\n",
    "    params = {}\n",
    "    if mode == \"test\":\n",
    "        params[\"index_col\"] = 0\n",
    "    if indices == \"all\":\n",
    "        pr = pd.read_csv(\"data/ItemPairs_%s.csv\" % mode, **params)\n",
    "    else:\n",
    "        pr = pd.read_csv(\"data/ItemPairs_%s.csv\" % mode, **params).loc[indices]\n",
    "\n",
    "    sourcecols = len(pr.columns)\n",
    "    for colname in columns:\n",
    "        if colname.startswith(\"pair_\"):\n",
    "            colname = colname[5:]\n",
    "            colfn = (\"data/ItemPairs_%s.csv\" % mode) + \"_\" + colname + \".csv\"\n",
    "            column = pd.read_csv(colfn, index_col=0)\n",
    "            pr = pr.merge(column[[colname]], how=\"left\", left_index=True, right_index=True, sort=False)\n",
    "            del column\n",
    "        elif colname == 'itemID':\n",
    "            pr[\"itemID_x\"] = pr[\"itemID_1\"]\n",
    "            pr[\"itemID_y\"] = pr[\"itemID_2\"]\n",
    "        else:\n",
    "            colfn = (\"data/ItemInfo_%s.csv\" % mode) + \"_\" + colname + \".csv\"\n",
    "            column = pd.read_csv(colfn, index_col=0)\n",
    "            if colname in (\"title\", \"description\", \"stoptitle\", \"stopdescription\"):\n",
    "                column = column.fillna(\"nan\")\n",
    "                column[column==\"\"] = \"nan\"\n",
    "            elif colname in (\"images_array\",):\n",
    "                column = column.fillna(\"\")\n",
    "            elif colname in (\"attrsJSON\",):\n",
    "                column = column.fillna(\"{}\")\n",
    "                column[column==\"\"] = \"{}\"\n",
    "            elif False and colname in (\"price\",):\n",
    "                column = column.fillna(0.0)\n",
    "                column = preprocess_price(column)\n",
    "            elif colname in (\"categoryID\", \"parentCategoryID\"):\n",
    "                column = column.fillna(0)\n",
    "            elif colname in (\"locationID\", \"regionID\", \"metroID\"):\n",
    "                column = column.fillna(1)\n",
    "            pr = pr.merge(column[[colname]], how=\"left\", left_on=\"itemID_1\", right_index=True, sort=False)\n",
    "            pr = pr.merge(column[[colname]], how=\"left\", left_on=\"itemID_2\", right_index=True, sort=False)\n",
    "            del column\n",
    "        if False and colname in (\"price\",):\n",
    "            pr = preprocess_pair_price(pr)\n",
    "#     print pr[pr.columns[sourcecols:]]\n",
    "    return pr[pr.columns[sourcecols:]].values\n",
    "\n",
    "@memory.cache\n",
    "def _get_transformer(mode, indices, columns, transformer):\n",
    "    params = {}\n",
    "    if mode == \"test\":\n",
    "        params[\"index_col\"] = 0\n",
    "    tr = clone(transformer)\n",
    "    output = tr.fit_transform(\n",
    "        _get_X(mode, indices, columns),\n",
    "        pd.read_csv(\"data/ItemPairs_%s.csv\" % mode, **params).loc[indices][\"isDuplicate\"].values)\n",
    "    return tr, output\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def _fit_transform(mode, indices, columns, transformer):\n",
    "    _, output = _get_transformer(\"train\", indices, columns, transformer)\n",
    "    return output\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def _transform(mode, train, test, columns, transformer):\n",
    "    tr, _ = _get_transformer(\"train\", train, columns, transformer)\n",
    "    output = tr.transform(_get_X(mode, test, columns))\n",
    "    return output\n",
    "\n",
    "def get_xgb_feat_importances(clf):\n",
    "    import xgboost as xgb\n",
    "\n",
    "    if isinstance(clf, xgb.XGBModel):\n",
    "        # clf has been created by calling\n",
    "        # xgb.XGBClassifier.fit() or xgb.XGBRegressor().fit()\n",
    "        fscore = clf.booster().get_fscore()\n",
    "    else:\n",
    "        # clf has been created by calling xgb.train.\n",
    "        # Thus, clf is an instance of xgb.Booster.\n",
    "        fscore = clf.get_fscore()\n",
    "\n",
    "    feat_importances = []\n",
    "    for ft, score in fscore.iteritems():\n",
    "        feat_importances.append({'Feature': ft, 'Importance': score})\n",
    "    feat_importances = pd.DataFrame(feat_importances)\n",
    "    feat_importances = feat_importances.sort_values(\n",
    "        by='Feature', ascending=True).reset_index(drop=True)\n",
    "    print feat_importances.head()\n",
    "    feat_importances = feat_importances.sort_values(\n",
    "        by='Importance', ascending=False).reset_index(drop=True)\n",
    "    # Divide the importances by the sum of all importances\n",
    "    # to get relative importances. By using relative importances\n",
    "    # the sum of all importances will equal to 1, i.e.,\n",
    "    # np.sum(feat_importances['importance']) == 1\n",
    "    feat_importances['Importance'] /= feat_importances['Importance'].sum()\n",
    "    # Print the most important features and their importances\n",
    "    print feat_importances.head()\n",
    "    return feat_importances\n",
    "\n",
    "def _get_first_col(X):\n",
    "    return X[:,[0]]\n",
    "\n",
    "def _binary_jaccard(X):\n",
    "    sm = X.sum(axis=1)\n",
    "    return np.hstack((\n",
    "            sm[:,:(sm.shape[-1]/2)].sum(axis=1),\n",
    "            sm[:,(sm.shape[-1]/2):].sum(axis=1),\n",
    "        )).astype(float) / sm\n",
    "\n",
    "def _price_diff(X):\n",
    "    return (np.nan_to_num(np.abs(X[:,0] - X[:,1])) / (np.nan_to_num(np.max(X[:,[0,1]], axis=1)) + 1.0)).reshape(-1,1)\n",
    "\n",
    "def _text_count(X):\n",
    "    a = CountVectorizer().build_analyzer()\n",
    "    return np.frompyfunc(lambda x: len(a(x)), 1, 1)(X.ravel()).reshape(X.shape).astype(float)\n",
    "\n",
    "def _text_concat(X):\n",
    "    xnew = np.zeros((X.shape[0], 2), dtype=X.dtype)\n",
    "    xnew[:,0] = X[:,0] + ' ' + X[:,2]\n",
    "    xnew[:,1] = X[:,1] + ' ' + X[:,3]\n",
    "    return xnew\n",
    "\n",
    "def _print_return(X):\n",
    "    print X\n",
    "    return X\n",
    "\n",
    "def _get_features(**kwargs):\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    if kwargs.get(\"pcats\", None) is None:\n",
    "        pcats = [1, 2, 4, 5, 6, 7, 8, 35, 110, 113]\n",
    "    else:\n",
    "        pcats = kwargs[\"pcats\"]\n",
    "    \n",
    "    if kwargs.get(\"cats\", None) is None:\n",
    "        cats = [9, 10, 11, 14, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 38, 39, 40, 42, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 101, 102, 105, 106, 111, 112, 114, 115, 116]\n",
    "    else:\n",
    "        cats = kwargs[\"cats\"]\n",
    "    \n",
    "    if kwargs.get(\"linear\", None) is None:\n",
    "        labelbin = []\n",
    "    else:\n",
    "        labelbin = [(\"labelvectorizer\", MyLabelBinarizer(sparse_output=True))]\n",
    "    \n",
    "    return [\n",
    "    ] + [\n",
    "        ([\"parentCategoryID\"], Pipeline([\n",
    "                    ('selector', Selector(include=[0])),\n",
    "                ] + labelbin)),\n",
    "        ([\"categoryID\"], Pipeline([\n",
    "                    ('selector', Selector(include=[0])),\n",
    "                ] + labelbin)),\n",
    "    ] + [\n",
    "        ([\"attrsJSON\"], Pipeline([\n",
    "                    ('jaccard', FunctionTransformer(_attrsJSON_dist, validate=False)),\n",
    "                ])),\n",
    "    ] + [\n",
    "#         ([\"categoryID\", \"attrsJSON\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "#                         ('feats', FeatureUnion(transformer_list=[\n",
    "#                                     ('attrs', Pipeline([\n",
    "#                                                 ('tr1', JsonToDict(1)),\n",
    "#                                                 ('tr2', JsonAttrsTrf(1)),\n",
    "#                                                 ('imputer', ConstantCol()),\n",
    "#                                             ])),\n",
    "#                                 ])),\n",
    "#                         ('est', LogisticRegression(C=0.01)),\n",
    "#                         ]), switch=0, val=categoryID, xcols=[2,3]))\n",
    "#         for categoryID in cats\n",
    "    ] + [\n",
    "        ([\"parentCategoryID\", \"attrsJSON\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "                        ('feats', FeatureUnion(transformer_list=[\n",
    "                                    ('attrs', Pipeline([\n",
    "                                                ('tr1', JsonToDict(1)),\n",
    "                                                ('tr2', JsonAttrsTrf(1)),\n",
    "                                                ('imputer', ConstantCol()),\n",
    "                                            ])),\n",
    "                                ])),\n",
    "                        ('est', LogisticRegression(C=0.01)),\n",
    "                        ]), switch=0, val=parentCategoryID, xcols=[2,3]))\n",
    "        for parentCategoryID in pcats\n",
    "    ] + [\n",
    "#         ([\"parentCategoryID\", \"stoptitle\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "#                         ('feats', FeatureUnion(transformer_list=[\n",
    "#                                     ('title', Pipeline([\n",
    "#                                                 ('transform', SwitchTransformer(est=CountVectorizer(min_df=10, binary=True), preaction=\"ravel\", postaction=[\"xor\"])),\n",
    "#                                             ])),\n",
    "#                                 ])),\n",
    "#                         ('est', SGDClassifier(loss=\"modified_huber\", alpha=0.001)),\n",
    "#                         ]), switch=0, val=parentCategoryID, xcols=[2,3]))\n",
    "#         for parentCategoryID in pcats\n",
    "    ] + [\n",
    "        ([\"parentCategoryID\", \"stoptitle\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "                        ('feats', FeatureUnion(transformer_list=[\n",
    "                                    ('title', Pipeline([\n",
    "                                                (\"transform\", SwitchTransformer(est=Pipeline([\n",
    "                                                                ('v', TfidfVectorizer(min_df=10, binary=True)),\n",
    "                                                                ('svd', TruncatedSVD(100)),\n",
    "                                                            ]), preaction=\"ravel\")),\n",
    "                                                (\"pairdist\", PairsDist(metrics=[\"cosine\", \"euclidean\", \"cityblock\"])),\n",
    "                                            ])),\n",
    "                                ])),\n",
    "                        ]), switch=0, val=parentCategoryID, xcols=[2,3]))\n",
    "        for parentCategoryID in pcats\n",
    "    ] + [\n",
    "#         ([\"parentCategoryID\", \"stoptitle\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "#                         ('feats', FeatureUnion(transformer_list=[\n",
    "#                                     ('title', Pipeline([\n",
    "#                                                 (\"transform\", SwitchTransformer(est=Pipeline([\n",
    "#                                                                 ('v', CountVectorizer(min_df=10, binary=True)),\n",
    "#                                                                 ('cl', SupervisedDecision(MiniBatchKMeans(500), method=\"predict\")),\n",
    "#                                                             ]), preaction=\"ravel\")),\n",
    "#                                                 (\"freq\", IdPairsTransformer()),\n",
    "#                                                 (\"binrz\", IdRecoder()),\n",
    "#                                             ])),\n",
    "#                                 ])),\n",
    "#                         ]), switch=0, val=parentCategoryID, xcols=[2,3]))\n",
    "#         for parentCategoryID in pcats\n",
    "    ] + [\n",
    "        ([\"parentCategoryID\", \"stoptitle\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "                        ('feats', FeatureUnion(transformer_list=[\n",
    "                                    ('title', Pipeline([\n",
    "                                                (\"transform\", SwitchTransformer(est=Pipeline([\n",
    "                                                                ('v', CountVectorizer(min_df=10, binary=True)),\n",
    "                                                                ('svd', TruncatedSVD(100)),\n",
    "                                                                ('cl', SupervisedDecision(MiniBatchKMeans(500), method=\"predict\")),\n",
    "                                                            ]), preaction=\"ravel\")),\n",
    "                                                (\"freq\", IdPairsTransformer()),\n",
    "                                                (\"binrz\", IdRecoder()),\n",
    "                                            ])),\n",
    "                                ])),\n",
    "                        ]), switch=0, val=parentCategoryID, xcols=[2,3]))\n",
    "        for parentCategoryID in pcats\n",
    "    ] + [\n",
    "        ([\"parentCategoryID\", \"stoptitle\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "                        ('feats', FeatureUnion(transformer_list=[\n",
    "                                    ('title', Pipeline([\n",
    "                                                ('transform', SwitchTransformer(est=CountVectorizer(analyzer=\"char_wb\", min_df=10, ngram_range=(1,5), binary=True), preaction=\"ravel\", postaction=[\"and\", \"xor\"])),\n",
    "                                            ])),\n",
    "                                ])),\n",
    "                        ('est', LogisticRegression(C=0.01)),\n",
    "                        ]), switch=0, val=parentCategoryID, xcols=[2,3]))\n",
    "        for parentCategoryID in pcats\n",
    "    ] + [\n",
    "#         ([\"categoryID\", \"stoptitle\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "#                         ('feats', FeatureUnion(transformer_list=[\n",
    "#                                     ('title', Pipeline([\n",
    "#                                                 ('transform', SwitchTransformer(est=CountVectorizer(min_df=10, binary=True), preaction=\"ravel\", postaction=[\"and\", \"xor\"])),\n",
    "#                                             ])),\n",
    "#                                 ])),\n",
    "#                         ('est', LogisticRegression(C=0.01)),\n",
    "#                         ]), switch=0, val=categoryID, xcols=[2,3]))\n",
    "#         for categoryID in cats\n",
    "    ] + [\n",
    "#         ([\"parentCategoryID\", \"stopdescription\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "#                         ('feats', FeatureUnion(transformer_list=[\n",
    "#                                     ('title', Pipeline([\n",
    "#                                                 ('transform', SwitchTransformer(est=CountVectorizer(min_df=10, binary=True), preaction=\"ravel\", postaction=[\"xor\"])),\n",
    "#                                             ])),\n",
    "#                                 ])),\n",
    "#                         ('est', LogisticRegression(C=0.01)),\n",
    "#                         ]), switch=0, val=parentCategoryID, xcols=[2,3]))\n",
    "#         for parentCategoryID in pcats\n",
    "    ] + [\n",
    "        ([\"parentCategoryID\", \"description\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "                        ('feats', FeatureUnion(transformer_list=[\n",
    "                                    ('desc', Pipeline([\n",
    "                                                ('transform', SwitchTransformer(est=CountVectorizer(binary=True, min_df=10), preaction=\"ravel\", postaction=[\"and\", \"xor\"])),\n",
    "                                            ])),\n",
    "                                ])),\n",
    "                        ('est', LogisticRegression(C=0.01)),\n",
    "                        ]), switch=0, val=parentCategoryID, xcols=[2,3]))\n",
    "        for parentCategoryID in pcats\n",
    "    ] + [\n",
    "#         ([\"categoryID\", \"description\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "#                         ('feats', FeatureUnion(transformer_list=[\n",
    "#                                     ('desc', Pipeline([\n",
    "#                                                 ('transform', SwitchTransformer(est=CountVectorizer(binary=True, min_df=10), preaction=\"ravel\", postaction=[\"and\", \"xor\"])),\n",
    "#                                             ])),\n",
    "#                                 ])),\n",
    "#                         ('est', LogisticRegression(C=0.01)),\n",
    "#                         ]), switch=0, val=categoryID, xcols=[2,3]))\n",
    "#         for categoryID in cats\n",
    "    ] + [\n",
    "#         ([\"title\"], Pipeline([\n",
    "#                     (\"titledist\", TitleDist()),\n",
    "#                 ])),\n",
    "#         ([\"stoptitle\"], Pipeline([\n",
    "#                     (\"titledist\", TitleDist()),\n",
    "#                 ])),\n",
    "        ([\"stoptitle\"], Pipeline([\n",
    "                    (\"titledist\", TitleDist(analyzer=CountVectorizer(min_df=10))),\n",
    "                ])),\n",
    "        ([\"stoptitle\"], Pipeline([\n",
    "                    (\"titledist\", TitleEditDist(n_jobs=2)),\n",
    "                ])),\n",
    "        ([\"stoptitle\"], Pipeline([\n",
    "                    (\"titledist\", TokenDist(n_jobs=2)),\n",
    "                ])),\n",
    "#         ([\"stoptitle\"], Pipeline([\n",
    "#                     ('title', Pipeline([\n",
    "#                                 (\"transform\", SwitchTransformer(est=Pipeline([\n",
    "#                                                 ('v', CountVectorizer(min_df=10, binary=True)),\n",
    "#                                                 ('cl', SupervisedDecision(MiniBatchKMeans(3000), method=\"predict\")),\n",
    "#                                             ]), preaction=\"ravel\")),\n",
    "#                                 (\"freq\", IdPairsTransformer()),\n",
    "#                                 (\"binrz\", IdRecoder()),\n",
    "#                             ])),\n",
    "#                 ])),\n",
    "#         ([\"title\"], Pipeline([\n",
    "#                     (\"titledist\", TitleDist(analyzer=CountVectorizer(analyzer=\"char_wb\", min_df=10, ngram_range=(1,4)))),\n",
    "#                 ])),\n",
    "        ([\"title\"], Pipeline([\n",
    "                    (\"titledist\", TitleDist(analyzer=CountVectorizer(analyzer=\"char_wb\", min_df=10, ngram_range=(1,5)))),\n",
    "                ])),\n",
    "    ] + [\n",
    "#         ([\"description\"], Pipeline([\n",
    "#                     (\"titledist\", TitleDist()),\n",
    "#                 ])),\n",
    "#         ([\"stopdescription\"], Pipeline([\n",
    "#                     (\"titledist\", TitleDist()),\n",
    "#                 ])),\n",
    "        ([\"stopdescription\"], Pipeline([\n",
    "                    (\"titledist\", TitleDist(analyzer=CountVectorizer(min_df=10))),\n",
    "                ])),\n",
    "        ([\"stopdescription\"], Pipeline([\n",
    "                    (\"titledist\", TitleEditDist(n_jobs=3)),\n",
    "                ])),\n",
    "        ([\"stopdescription\"], Pipeline([\n",
    "                    (\"titledist\", TokenDist(n_jobs=3)),\n",
    "                ])),\n",
    "#         ([\"description\"], Pipeline([\n",
    "#                     (\"titledist\", TitleDist(analyzer=CountVectorizer(analyzer=\"char_wb\", min_df=10, ngram_range=(1,4)))),\n",
    "#                 ])),\n",
    "#         ([\"description\"], Pipeline([\n",
    "#                     (\"titledist\", TitleDist(analyzer=CountVectorizer(analyzer=\"char_wb\", min_df=10, ngram_range=(1,5)))),\n",
    "#                 ])),\n",
    "    ] + [\n",
    "#         ([\"title\", \"description\"], Pipeline([\n",
    "#                     (\"concat\", FunctionTransformer(_text_concat, validate=False)),\n",
    "#                     (\"titledist\", TitleDist()),\n",
    "#                 ])),\n",
    "        ([\"stoptitle\", \"stopdescription\"], Pipeline([\n",
    "                    (\"concat\", FunctionTransformer(_text_concat, validate=False)),\n",
    "                    (\"titledist\", TitleDist()),\n",
    "                ])),\n",
    "        ([\"stoptitle\", \"stopdescription\"], Pipeline([\n",
    "                    (\"concat\", FunctionTransformer(_text_concat, validate=False)),\n",
    "                    (\"editdist\", TitleEditDist()),\n",
    "                ])),\n",
    "    ] + [\n",
    "#         ([\"title\"], Pipeline([\n",
    "#                     (\"counter\", FunctionTransformer(_text_count, validate=False)),\n",
    "#                     (\"freq\", IdPairsTransformer()),\n",
    "#                     (\"binrz\", IdRecoder()),\n",
    "#                 ] + labelbin)),\n",
    "        ([\"stoptitle\"], Pipeline([\n",
    "                    (\"counter\", FunctionTransformer(_text_count, validate=False)),\n",
    "                    (\"freq\", IdPairsTransformer()),\n",
    "                    (\"binrz\", IdRecoder()),\n",
    "                ] + labelbin)),\n",
    "#         ([\"description\"], Pipeline([\n",
    "#                     (\"counter\", FunctionTransformer(_text_count, validate=False)),\n",
    "#                     (\"freq\", IdPairsTransformer()),\n",
    "#                     (\"binrz\", IdRecoder()),\n",
    "#                 ] + labelbin)),\n",
    "        ([\"stopdescription\"], Pipeline([\n",
    "                    (\"counter\", FunctionTransformer(_text_count, validate=False)),\n",
    "                    (\"freq\", IdPairsTransformer()),\n",
    "                    (\"binrz\", IdRecoder()),\n",
    "                ] + labelbin)),\n",
    "        ([\"images_array\"], Pipeline([\n",
    "                    (\"counter\", FunctionTransformer(_text_count, validate=False)),\n",
    "                    (\"freq\", IdPairsTransformer()),\n",
    "                    (\"binrz\", IdRecoder()),\n",
    "                ] + labelbin)),\n",
    "    ] + [\n",
    "        ([\"itemID\"], Pipeline([\n",
    "                    ('freq', FreqTransformer()),\n",
    "                    ('pairs', IdPairsTransformer()),\n",
    "                    ('binrz', IdRecoder()),\n",
    "                ])),\n",
    "        ([\"locationID\"], SwitchTransformer(est=Pipeline([\n",
    "                        ('recored', IdRecoder()),\n",
    "                    ] + labelbin))),\n",
    "        ([\"locationID\", \"lat\", \"lon\"], LocationDist(list(np.logspace(-3.5, 6, 20, base=2)))),\n",
    "        ([\"regionID\"], SwitchTransformer(est=Pipeline([\n",
    "                        ('recored', IdRecoder()),\n",
    "                    ] + labelbin))),\n",
    "        ([\"regionID\", \"lat\", \"lon\"], LocationDist(list(np.logspace(-2, 7, 20, base=2)))),\n",
    "        ([\"metroID\"], SwitchTransformer(est=Pipeline([\n",
    "                        ('recored', IdRecoder()),\n",
    "                    ] + labelbin))),\n",
    "        ([\"metroID\", \"lat\", \"lon\"], LocationDist(list(np.logspace(-4, 4, 5, base=2)))),\n",
    "        ([\"lat\", \"lon\"], CoordDist()),\n",
    "        ([\"lat\", \"lon\"], VarianceThreshold()),\n",
    "        ([\"locationID\"], Pipeline([\n",
    "                    ('freq', IdPairsTransformer()),\n",
    "                    ('binrz', IdRecoder()),\n",
    "                ] + labelbin)),\n",
    "        ([\"locationID\", \"lat\", \"lon\"], Pipeline([\n",
    "                    ('predict', CoordAnchor()),\n",
    "                    ('combine', IdPairsTransformer()),\n",
    "                    ('binrz', IdRecoder()),\n",
    "                ] + labelbin)),\n",
    "#         ([\"locationID\"], Pipeline([\n",
    "#                     ('freq', IdFreq(1000)),\n",
    "#                     ('binrz', EqNonEqBinarizer()),\n",
    "#                 ])),\n",
    "#         ([\"locationID\"], SupervisedDecision(est=Pipeline([\n",
    "#                         ('binrz', EqNonEqBinarizer()),\n",
    "#                         ('est', LogisticRegression(C=1.0)),\n",
    "#                 ]))),\n",
    "        ([\"regionID\", \"lat\", \"lon\"], Pipeline([\n",
    "                    ('predict', CoordAnchor()),\n",
    "                    ('freq', IdPairsTransformer()),\n",
    "                    ('binrz', IdRecoder()),\n",
    "                ] + labelbin)),\n",
    "        ([\"regionID\"], Pipeline([\n",
    "                    ('freq', IdPairsTransformer()),\n",
    "                    ('binrz', IdRecoder()),\n",
    "                ] + labelbin)),\n",
    "#         ([\"regionID\"], Pipeline([\n",
    "#                     ('binrz', EqNonEqBinarizer()),\n",
    "#                 ])),\n",
    "        ([\"metroID\", \"lat\", \"lon\"], Pipeline([\n",
    "                    ('predict', CoordAnchor()),\n",
    "                    ('freq', IdPairsTransformer()),\n",
    "                    ('binrz', IdRecoder()),\n",
    "                ] + labelbin)),\n",
    "#         ([\"metroID\"], SupervisedDecision(est=Pipeline([\n",
    "#                         ('binrz', EqNonEqBinarizer()),\n",
    "#                         ('est', LogisticRegression(C=1.0)),\n",
    "#                 ]))),\n",
    "        ([\"metroID\"], Pipeline([\n",
    "                    ('freq', IdPairsTransformer()),\n",
    "                    ('binrz', IdRecoder()),\n",
    "                ] + labelbin)),\n",
    "#         ([\"metroID\"], Pipeline([\n",
    "#                     ('binrz', EqNonEqBinarizer()),\n",
    "#                 ])),\n",
    "        ([\"price\"], Pipeline([\n",
    "                    ('trf', FunctionTransformer(_price_diff, validate=False)),\n",
    "                ])),\n",
    "#         ([\"price\"], Pipeline([\n",
    "#                     ('binrz', EqNonEqBinarizer()),\n",
    "#                 ])),\n",
    "        ([\"price\"], Pipeline([\n",
    "                    ('trf', PriceTransformer()),\n",
    "                    ('pairs', IdPairsTransformer()),\n",
    "                    ('recoder', IdRecoder()),\n",
    "                    ('noop', VarianceThreshold()),\n",
    "                ] + labelbin)),\n",
    "        ([\"categoryID\", \"price\"], Pipeline([\n",
    "                    ('selector', Selector(include=[1,2,3])),\n",
    "                    ('trf', PriceOutlierTransformer()),\n",
    "                ])),\n",
    "        ([\"parentCategoryID\", \"price\"], Pipeline([\n",
    "                    ('selector', Selector(include=[1,2,3])),\n",
    "                    ('trf', PriceOutlierTransformer()),\n",
    "                ])),\n",
    "        ([\"pair_imagesdist_Exact\"], VarianceThreshold()),\n",
    "        ([\"pair_imagesdist_Correlation_max\"], Imputer()),\n",
    "        ([\"pair_imagesdist_Chi-Squared_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_Intersection_max\"], Imputer()),\n",
    "        ([\"pair_imagesdist_Hellinger_min\"], Imputer()),\n",
    "\n",
    "        ([\"pair_imagesdist_Correlation_mean\"], Imputer()),\n",
    "        ([\"pair_imagesdist_Chi-Squared_mean\"], Imputer()),\n",
    "        ([\"pair_imagesdist_Intersection_mean\"], Imputer()),\n",
    "        ([\"pair_imagesdist_Hellinger_mean\"], Imputer()),\n",
    "\n",
    "        ([\"pair_imagesdist_scipy.braycurtis_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_scipy.canberra_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_scipy.chebyshev_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_scipy.cityblock_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_scipy.cosine_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_scipy.euclidean_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_scipy.hamming_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_scipy.sqeuclidean_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_skimage.compare_mse_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_skimage.compare_ssim_3_max\"], Imputer()),\n",
    "        ([\"pair_imagesdist_skimage.compare_ssim_5_max\"], Imputer()),\n",
    "        ([\"pair_imagesdist_skimage.compare_ssim_7_max\"], Imputer()),\n",
    "        ([\"pair_imagesdist_skimage.compare_mse.adapt_min\"], Imputer()),\n",
    "        ([\"pair_imagesdist_skimage.compare_ssim_3.adapt_max\"], Imputer()),\n",
    "        ([\"pair_imagesdist_skimage.compare_ssim_5.adapt_max\"], Imputer()),\n",
    "        ([\"pair_imagesdist_skimage.compare_ssim_7.adapt_max\"], Imputer()),\n",
    "\n",
    "#         ([\"pair_imagesdist_scipy.braycurtis_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_scipy.canberra_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_scipy.chebyshev_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_scipy.cityblock_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_scipy.cosine_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_scipy.euclidean_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_scipy.hamming_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_scipy.sqeuclidean_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_skimage.compare_mse_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_skimage.compare_ssim_3_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_skimage.compare_ssim_5_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_skimage.compare_ssim_7_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_skimage.compare_mse.adapt_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_skimage.compare_ssim_3.adapt_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_skimage.compare_ssim_5.adapt_mean\"], Imputer()),\n",
    "#         ([\"pair_imagesdist_skimage.compare_ssim_7.adapt_mean\"], Imputer()),\n",
    "    ] + [\n",
    "        ([\"pair_imagesdist_Correlation_%d\" % _], VarianceThreshold())\n",
    "        for _ in range(99, 97, -1)\n",
    "#         for _ in [95, 90, 85, 75, 70, 65, 60]\n",
    "#         for _ in [95, 90, 85]\n",
    "    ] + [\n",
    "        ([\"pair_imagesdist_Chi-Squared_%d\" % _], VarianceThreshold())\n",
    "        for _ in range(1,3)\n",
    "#         for _ in [10, 20, 50, 100, 150, 200]\n",
    "    ] + [\n",
    "        ([\"pair_imagesdist_Intersection_%d\" % _], VarianceThreshold())\n",
    "        for _ in range(250,230,-10)\n",
    "    ] + [\n",
    "        ([\"pair_imagesdist_Hellinger_%d\" % _], VarianceThreshold())\n",
    "        for _ in range(1,3)\n",
    "#         for _ in [10, 20]\n",
    "#     ] + [\n",
    "#         ([\"parentCategoryID\", \"attrsJSON\"], FilteredSupervisedDecision(est=Pipeline([\n",
    "#                         ('feats', FeatureUnion(transformer_list=[\n",
    "#                                     ('attrs', Pipeline([\n",
    "#                                                 ('transform', SwitchTransformer(est=CountVectorizer(binary=True), preaction=\"ravel\", postaction=[\"and\", \"xor\"])),\n",
    "#                                             ])),\n",
    "#                                 ])),\n",
    "#                         ('est', LogisticRegression(C=0.01)),\n",
    "#                         ]), switch=0, val=parentCategoryID, xcols=[2,3]))\n",
    "#         for parentCategoryID in [1, 2, 4, 5, 6, 7, 8, 35, 110, 113]\n",
    "    ]\n",
    "\n",
    "\n",
    "def _get_estimator():\n",
    "    from xgboost import XGBClassifier\n",
    "    return XGBClassifier(\n",
    "        n_estimators=int(os.environ.get(\"N_ESTIMATORS\", \"900\")),\n",
    "        max_depth=int(os.environ.get(\"MAX_DEPTH\", \"9\"))\n",
    "    )\n",
    "    return XGBClassifier(n_estimators=1500, max_depth=5)\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    return LogisticRegression(C=1.0, random_state=251)\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    return RandomForestClassifier(n_estimators=700, max_depth=9, n_jobs=-1)\n",
    "\n",
    "\n",
    "def _get_conf():\n",
    "    return (\n",
    "        [\n",
    "            ([\"meta\"], [\n",
    "                    ([\"title\"], Pipeline([\n",
    "                                ('transform', SwitchTransformer(est=CountVectorizer(binary=True), preaction=\"ravel\", postaction=[\"and\", \"xor\"])),\n",
    "                            ])),\n",
    "                ], SupervisedDecision(est=LogisticRegression(C=1.0)))\n",
    "        ] + [\n",
    "            \n",
    "        ],\n",
    "        SupervisedDecision(est=XGBClassifier(n_estimators=100)))\n",
    "\n",
    "\n",
    "def newcv(n_folds=5):\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.cross_validation import StratifiedKFold\n",
    "    from sklearn.cross_validation import cross_val_score\n",
    "    from sklearn.metrics import get_scorer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.utils import shuffle\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    pr = pd.read_csv(\"data/ItemPairs_train.csv\")\n",
    "\n",
    "    kf = StratifiedKFold(pr[\"isDuplicate\"].values, n_folds=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    feats = _get_features()\n",
    "    scorer = get_scorer(\"roc_auc\")\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for train, test in kf:\n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        itrain = shuffle(pr.iloc[train].index, random_state=42)\n",
    "        itest = pr.iloc[test].index\n",
    "        \n",
    "        Xs = [_fit_transform(\"train\", itrain, columns, transformer) for columns, transformer in feats]\n",
    "        if any(sp.issparse(f) for f in Xs):\n",
    "            Xs = sp.hstack(Xs).tocsr()\n",
    "        else:\n",
    "            Xs = np.hstack(Xs)\n",
    "\n",
    "        print \"Shape:\", Xs.shape\n",
    "        est = clone(_get_estimator())\n",
    "        est.fit(Xs, pr.loc[itrain][\"isDuplicate\"])\n",
    "        del Xs\n",
    "        \n",
    "        Xs = [_transform(\"train\", itrain, itest, columns, transformer) for columns, transformer in feats]\n",
    "        if any(sp.issparse(f) for f in Xs):\n",
    "            Xs = sp.hstack(Xs).tocsr()\n",
    "        else:\n",
    "            Xs = np.hstack(Xs)\n",
    "        \n",
    "        scores.append(scorer(est, Xs, pr.loc[itest][\"isDuplicate\"]))\n",
    "        \n",
    "        print \"CV:\", (datetime.datetime.now() - start_time), Xs.shape, scores[-1]\n",
    "\n",
    "        del Xs\n",
    "        del est\n",
    "        \n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def newtest(**kwargs):\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.utils import shuffle\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    prtrain = pd.read_csv(\"data/ItemPairs_train.csv\")\n",
    "#     prtrain = pd.merge(prtrain, pd.read_csv(\"data/ItemInfo_train.csv_categoryID.csv\", index_col=0), left_on=\"itemID_1\", right_index=True, how=\"inner\", sort=False)\n",
    "#     prtrain.drop(prtrain[prtrain.categoryID != 9].index, inplace=True)\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    if kwargs.get(\"trainidx\", None) is None:\n",
    "        indices = shuffle(prtrain.index, random_state=42)\n",
    "    else:\n",
    "        indices = kwargs[\"trainidx\"]\n",
    "    prtrain = prtrain.loc[indices]\n",
    "\n",
    "    if kwargs.get(\"feats\", None) is None:\n",
    "        feats = _get_features()\n",
    "    else:\n",
    "        feats = kwargs[\"feats\"]\n",
    "\n",
    "    featsum = []\n",
    "#     Xs = [_fit_transform(\"train\", indices, columns, transformer) for columns, transformer in feats]\n",
    "#     Xs = []\n",
    "    Xsparse = []\n",
    "    featscount = 0\n",
    "    for columns, transformer in feats:\n",
    "        x = _fit_transform(\"train\", indices, columns, transformer)\n",
    "        print columns, x.shape\n",
    "        featsum.append([columns, x.shape, ['f%d' % _ for _ in range(featscount, featscount+x.shape[1])]])\n",
    "        Xsparse.append(sp.issparse(x))\n",
    "        featscount += x.shape[1]\n",
    "        del x\n",
    "    print(\"FITTED\")\n",
    "\n",
    "    Xs = None\n",
    "    for columns, transformer in feats:\n",
    "        print(\"Loading: %s\" % str(columns))\n",
    "        if Xs is None:\n",
    "            Xs = _fit_transform(\"train\", indices, columns, transformer).astype(float)\n",
    "            if any(Xsparse):\n",
    "                Xs = sp.lil_matrix(Xs)\n",
    "        else:\n",
    "            if any(Xsparse):\n",
    "                Xs = sp.hstack((\n",
    "                        Xs,\n",
    "                        _fit_transform(\"train\", indices, columns, transformer).astype(float)\n",
    "                    ))\n",
    "            else:\n",
    "                Xs = np.hstack((\n",
    "                        Xs,\n",
    "                        _fit_transform(\"train\", indices, columns, transformer).astype(float)\n",
    "                    ))\n",
    "    if sp.issparse(Xs):\n",
    "        Xs = Xs.tocsr()\n",
    "\n",
    "    print \"Shape:\", Xs.shape\n",
    "    \n",
    "    if os.environ.get(\"FNAMES\", \"\") != \"\":\n",
    "        for fname in os.environ[\"FNAMES\"].split(\",\"):\n",
    "            ff = pd.read_csv(\"features/%s.csv\" % fname, index_col=range(2))\n",
    "            fcolnames = [_ for _ in ff.columns if not _.endswith(\"_x\") and not _.endswith(\"_y\")]\n",
    "            ff = pd.merge(\n",
    "                prtrain.loc[indices],\n",
    "                ff,\n",
    "                left_on=[\"itemID_1\", \"itemID_2\"],\n",
    "                right_index=True,\n",
    "                how=\"left\", sort=False\n",
    "            )\n",
    "            Xs = sp.hstack((\n",
    "                    Xs,\n",
    "                    ff.loc[indices][fcolnames].values.astype(float)\n",
    "                ))\n",
    "        print \"FNAMES Shape:\", Xs.shape\n",
    "\n",
    "    if kwargs.get(\"est\", None) is None:\n",
    "        est = clone(_get_estimator())\n",
    "    else:\n",
    "        est = clone(kwargs[\"est\"])\n",
    "    \n",
    "    gc.collect()\n",
    "    if kwargs.get(\"fitidx\", None) is None:\n",
    "        fitidx = indices\n",
    "        if os.environ.get(\"STRATIFIED\", None) is not None:\n",
    "            fitidx = shuffle(get_train_indices(os.environ.get(\"STRATIFIED\")), random_state=42)\n",
    "    else:\n",
    "        fitidx = kwargs[\"fitidx\"]\n",
    "    \n",
    "    if len(fitidx) != len(indices) or (fitidx != indices).all():\n",
    "        mask = indices.isin(fitidx)\n",
    "        print \"Fit Shape:\", Xs[mask].shape\n",
    "        est.fit(Xs[mask], prtrain.loc[mask][\"isDuplicate\"])\n",
    "    else:\n",
    "        est.fit(Xs, prtrain.loc[indices][\"isDuplicate\"])\n",
    "\n",
    "    if os.environ.get(\"RESTRAIN\", None) is not None:\n",
    "        prtrain[\"probability\"] = est.predict_proba(Xs)[:,1]\n",
    "        with gzip.open(os.environ.get(\"RESTRAIN\"), \"wb\") as f:\n",
    "            prtrain[[\"probability\"]].to_csv(f, index_label=\"id\")\n",
    "    del Xs\n",
    "    \n",
    "    try:\n",
    "        fi = get_xgb_feat_importances(est)\n",
    "        featsum = [(fi[fi.Feature.isin(_[2])][\"Importance\"].sum(), _[0], _[1]) for _ in featsum]\n",
    "        for _ in sorted(featsum, key=lambda x: x[0], reverse=True):\n",
    "            print \"%.06f\" % _[0], _[1], _[2]\n",
    "#         for findex, fscore in iter(fi.values[:50]):\n",
    "#             print fscore, featsum[int(findex.replace('f', ''))]\n",
    "    except:\n",
    "        # not xgb\n",
    "        import sys\n",
    "        sys.excepthook(*sys.exc_info())\n",
    "\n",
    "    prtest = pd.read_csv(\"data/ItemPairs_test.csv\", index_col=0)\n",
    "#     prtest = pd.merge(prtest, pd.read_csv(\"data/ItemInfo_test.csv_categoryID.csv\", index_col=0), left_on=\"itemID_1\", right_index=True, how=\"inner\", sort=False)\n",
    "#     prtest.drop(prtest[prtest.categoryID != 9].index, inplace=True)\n",
    "\n",
    "    if kwargs.get(\"testidx\", None) is None:\n",
    "        testidx = prtest.index\n",
    "    else:\n",
    "        testidx = kwargs[\"testidx\"]\n",
    "        prtest = prtest.loc[testidx]\n",
    "\n",
    "    Xsparse = []\n",
    "    for columns, transformer in feats:\n",
    "        x = _transform(\"test\", indices, testidx, columns, transformer)\n",
    "        print columns, x.shape\n",
    "        Xsparse.append(sp.issparse(x))\n",
    "        del x\n",
    "        \n",
    "    Xs = None\n",
    "    for columns, transformer in feats:\n",
    "        if Xs is None:\n",
    "            Xs = _transform(\"test\", indices, testidx, columns, transformer).astype(float)\n",
    "            if any(Xsparse):\n",
    "                Xs = sp.csr_matrix(Xs)\n",
    "        else:\n",
    "            if any(Xsparse):\n",
    "                Xs = sp.hstack((\n",
    "                        Xs,\n",
    "                        _transform(\"test\", indices, testidx, columns, transformer).astype(float)\n",
    "                    )).tocsr()\n",
    "            else:\n",
    "                Xs = np.hstack((\n",
    "                        Xs,\n",
    "                        _transform(\"test\", indices, testidx, columns, transformer).astype(float)\n",
    "                    ))\n",
    "    \n",
    "    del prtrain\n",
    "    \n",
    "    if os.environ.get(\"FNAMES\", \"\") != \"\":\n",
    "        for fname in os.environ[\"FNAMES\"].split(\",\"):\n",
    "            ff = pd.read_csv(\"features/%s_test.csv\" % fname, index_col=range(2))\n",
    "            fcolnames = [_ for _ in ff.columns if not _.endswith(\"_x\") and not _.endswith(\"_y\")]\n",
    "            ff = pd.merge(\n",
    "                prtest.loc[testidx],\n",
    "                ff,\n",
    "                left_on=[\"itemID_1\", \"itemID_2\"],\n",
    "                right_index=True,\n",
    "                how=\"left\", sort=False\n",
    "            )\n",
    "            Xs = sp.hstack((\n",
    "                    Xs,\n",
    "                    ff.loc[testidx][fcolnames].values.astype(float)\n",
    "                ))\n",
    "        print \"FNAMES Shape:\", Xs.shape\n",
    "    \n",
    "    prtest[\"probability\"] = est.predict_proba(Xs)[:,1]\n",
    "    with gzip.open(os.environ.get(\"RESTEST\", \"/tmp/avito.csv.gz\"), \"wb\") as f:\n",
    "        prtest[[\"probability\"]].to_csv(f)\n",
    "\n",
    "    print \"TEST:\", (datetime.datetime.now() - start_time), Xs.shape\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def _score(**kwargs):\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import get_scorer\n",
    "    from sklearn.utils import shuffle\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "    prtrain = pd.read_csv(\"data/ItemPairs_train.csv\")\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    if kwargs.get(\"trainidx\", None) is None:\n",
    "        indices = shuffle(prtrain.index, random_state=42)\n",
    "        if os.environ.get(\"STRATIFIED\", None) is not None:\n",
    "            indices = shuffle(get_train_indices(os.environ.get(\"STRATIFIED\")), random_state=42)\n",
    "    else:\n",
    "        indices = kwargs[\"trainidx\"]\n",
    "    prtrain = prtrain.loc[indices]\n",
    "\n",
    "    if kwargs.get(\"feats\", None) is None:\n",
    "        feats = _get_features()\n",
    "    else:\n",
    "        feats = kwargs[\"feats\"]\n",
    "\n",
    "    featsum = []\n",
    "#     Xs = [_fit_transform(\"train\", indices, columns, transformer) for columns, transformer in feats]\n",
    "#     Xs = []\n",
    "    Xsparse = []\n",
    "    featscount = 0\n",
    "    for columns, transformer in feats:\n",
    "        x = _fit_transform(\"train\", indices, columns, transformer)\n",
    "        print columns, x.shape\n",
    "        featsum.append([columns, x.shape, ['f%d' % _ for _ in range(featscount, featscount+x.shape[1])]])\n",
    "        Xsparse.append(sp.issparse(x))\n",
    "        featscount += x.shape[1]\n",
    "        del x\n",
    "    print(\"FITTED\")\n",
    "\n",
    "    Xs = None\n",
    "    for columns, transformer in feats:\n",
    "        print(\"Loading: %s\" % str(columns))\n",
    "        if Xs is None:\n",
    "            Xs = _fit_transform(\"train\", indices, columns, transformer).astype(float)\n",
    "            if any(Xsparse):\n",
    "                Xs = sp.lil_matrix(Xs)\n",
    "        else:\n",
    "            if any(Xsparse):\n",
    "                Xs = sp.hstack((\n",
    "                        Xs,\n",
    "                        _fit_transform(\"train\", indices, columns, transformer).astype(float)\n",
    "                    ))\n",
    "            else:\n",
    "                Xs = np.hstack((\n",
    "                        Xs,\n",
    "                        _fit_transform(\"train\", indices, columns, transformer).astype(float)\n",
    "                    ))\n",
    "    if sp.issparse(Xs):\n",
    "        Xs = Xs.tocsr()\n",
    "\n",
    "    print \"Shape:\", Xs.shape\n",
    "    \n",
    "    if kwargs.get(\"est\", None) is None:\n",
    "        est = clone(_get_estimator())\n",
    "    else:\n",
    "        est = clone(kwargs[\"est\"])\n",
    "    \n",
    "    if kwargs.get(\"fitidx\", None) is None:\n",
    "        est.fit(Xs, prtrain.loc[indices][\"isDuplicate\"])\n",
    "    else:\n",
    "        fitidx = kwargs[\"fitidx\"]\n",
    "        mask = indices.isin(fitidx)\n",
    "        est.fit(Xs[mask], prtrain.loc[mask][\"isDuplicate\"])\n",
    "#     if os.environ.get(\"RESTRAIN\", None) is not None:\n",
    "#         prtrain[\"probability\"] = est.predict_proba(Xs)[:,1]\n",
    "#         with gzip.open(os.environ.get(\"RESTRAIN\"), \"wb\") as f:\n",
    "#             prtrain[[\"probability\"]].to_csv(f, index_label=\"id\")\n",
    "    del Xs\n",
    "    \n",
    "    prtest = pd.read_csv(\"data/ItemPairs_train.csv\")\n",
    "\n",
    "    if kwargs.get(\"testidx\", None) is None:\n",
    "        testidx = prtest.index\n",
    "    else:\n",
    "        testidx = kwargs[\"testidx\"]\n",
    "        prtest = prtest.loc[testidx]\n",
    "\n",
    "    Xsparse = []\n",
    "    for columns, transformer in feats:\n",
    "        x = _transform(\"train\", indices, testidx, columns, transformer)\n",
    "        print columns, x.shape\n",
    "        Xsparse.append(sp.issparse(x))\n",
    "        del x\n",
    "        \n",
    "    Xs = None\n",
    "    for columns, transformer in feats:\n",
    "        if Xs is None:\n",
    "            Xs = _transform(\"train\", indices, testidx, columns, transformer).astype(float)\n",
    "            if any(Xsparse):\n",
    "                Xs = sp.csr_matrix(Xs)\n",
    "        else:\n",
    "            if any(Xsparse):\n",
    "                Xs = sp.hstack((\n",
    "                        Xs,\n",
    "                        _transform(\"train\", indices, testidx, columns, transformer).astype(float)\n",
    "                    )).tocsr()\n",
    "            else:\n",
    "                Xs = np.hstack((\n",
    "                        Xs,\n",
    "                        _transform(\"train\", indices, testidx, columns, transformer).astype(float)\n",
    "                    ))\n",
    "    \n",
    "    del prtrain\n",
    "    \n",
    "#     prtest[\"probability\"] = est.predict_proba(Xs)[:,1]\n",
    "#     with gzip.open(os.environ.get(\"RESTEST\", \"/tmp/avito.csv.gz\"), \"wb\") as f:\n",
    "#         prtest[[\"probability\"]].to_csv(f)\n",
    "\n",
    "    score = get_scorer(\"roc_auc\")(est, Xs, prtest[\"isDuplicate\"].values)\n",
    "    \n",
    "    print \"TEST:\", (datetime.datetime.now() - start_time), Xs.shape\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def _pcattest():\n",
    "    import gc\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    from sklearn.grid_search import ParameterGrid\n",
    "    \n",
    "    prtrain = pd.read_csv(\"data/ItemPairs_train.csv\")\n",
    "    prtrain = pd.merge(\n",
    "        prtrain,\n",
    "        pd.read_csv(\"data/ItemInfo_train.csv_parentCategoryID.csv\", index_col=0),\n",
    "        left_on=\"itemID_1\", right_index=True,\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "\n",
    "    prtest = pd.read_csv(\"data/ItemPairs_test.csv\", index_col=0)\n",
    "    prtest = pd.merge(\n",
    "        prtest,\n",
    "        pd.read_csv(\"data/ItemInfo_test.csv_parentCategoryID.csv\", index_col=0),\n",
    "        left_on=\"itemID_1\", right_index=True,\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "    \n",
    "    cats = pd.read_csv(\"data/Category.csv\", index_col=0)\n",
    "    \n",
    "    for pcatid in np.unique(prtrain[\"parentCategoryID\"].values):\n",
    "        print \"PARENT CATEGORY: %d\" % pcatid\n",
    "        os.environ[\"RESTRAIN\"] = \"/tmp/avito-pcat-%d-train.csv.gz\" % pcatid\n",
    "        os.environ[\"RESTEST\"] = \"/tmp/avito-pcat-%d-test.csv.gz\" % pcatid\n",
    "        if all((os.path.exists(os.environ[\"RESTRAIN\"]),\n",
    "                os.path.exists(os.environ[\"RESTEST\"]),\n",
    "                os.environ.get(\"RECALC\", None) is None)):\n",
    "            continue\n",
    "\n",
    "        scores = []\n",
    "        fulltrain = shuffle(prtrain[prtrain[\"parentCategoryID\"] == pcatid].index, random_state=42)\n",
    "        fulltest = prtest[prtest[\"parentCategoryID\"] == pcatid].index\n",
    "        train, test = train_test_split(fulltrain, test_size=1.0 * len(fulltest) / len(fulltrain), random_state=42)\n",
    "        param_grid = ParameterGrid({\n",
    "                \"n_estimators\": [50, 100, 200, 300, 500, 700],\n",
    "                \"max_depth\": [3, 4, 5, 6, 7],\n",
    "            })\n",
    "        param_grid = ParameterGrid({\n",
    "                \"n_estimators\": [50, 100, 200, 300, 500],\n",
    "                \"max_depth\": [3, 4, 5],\n",
    "            })\n",
    "        \n",
    "        for param_set in param_grid:\n",
    "            scores.append({\n",
    "                    \"params\": param_set,\n",
    "                    \"score\": _score(\n",
    "                        trainidx=train,\n",
    "                        testidx=test,\n",
    "                        feats = _get_features(\n",
    "                            pcats=[pcatid],\n",
    "                            cats=list(cats[cats[\"parentCategoryID\"] == pcatid].index)\n",
    "                        ),\n",
    "                        est=XGBClassifier(**param_set))\n",
    "                })\n",
    "            print scores[-1]\n",
    "        \n",
    "        scores = sorted(scores, key=lambda x: x[\"score\"], reverse=True)\n",
    "        for score in scores:\n",
    "            print score[\"score\"], score[\"params\"]\n",
    "        \n",
    "        newtest(\n",
    "            trainidx=fulltrain,\n",
    "            testidx=fulltest,\n",
    "            feats = _get_features(\n",
    "                pcats=[pcatid],\n",
    "                cats=list(cats[cats[\"parentCategoryID\"] == pcatid].index)\n",
    "            ),\n",
    "            est=XGBClassifier(**scores[0][\"params\"])\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def _cattest():\n",
    "    import gc\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    from sklearn.grid_search import ParameterGrid\n",
    "    \n",
    "    prtrain = pd.read_csv(\"data/ItemPairs_train.csv\")\n",
    "    prtrain = pd.merge(\n",
    "        prtrain,\n",
    "        pd.read_csv(\"data/ItemInfo_train.csv_categoryID.csv\", index_col=0),\n",
    "        left_on=\"itemID_1\", right_index=True,\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "\n",
    "    prtest = pd.read_csv(\"data/ItemPairs_test.csv\", index_col=0)\n",
    "    prtest = pd.merge(\n",
    "        prtest,\n",
    "        pd.read_csv(\"data/ItemInfo_test.csv_categoryID.csv\", index_col=0),\n",
    "        left_on=\"itemID_1\", right_index=True,\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "    \n",
    "    cats = pd.read_csv(\"data/Category.csv\", index_col=0)\n",
    "    \n",
    "    for catid in np.unique(prtrain[\"categoryID\"].values):\n",
    "        print \"CATEGORY: %d\" % catid\n",
    "        os.environ[\"RESTRAIN\"] = \"/tmp/avito-cat-%d-train.csv.gz\" % catid\n",
    "        os.environ[\"RESTEST\"] = \"/tmp/avito-cat-%d-test.csv.gz\" % catid\n",
    "        if all((os.path.exists(os.environ[\"RESTRAIN\"]),\n",
    "                os.path.exists(os.environ[\"RESTEST\"]),\n",
    "                os.environ.get(\"RECALC\", None) is None)):\n",
    "            continue\n",
    "        \n",
    "        scores = []\n",
    "        fulltrain = shuffle(prtrain[prtrain[\"categoryID\"] == catid].index, random_state=42)\n",
    "        fulltest = prtest[prtest[\"categoryID\"] == catid].index\n",
    "        train, test = train_test_split(fulltrain, test_size=1.0 * len(fulltest) / len(fulltrain), random_state=42)\n",
    "        param_grid = ParameterGrid({\n",
    "                \"n_estimators\": [30, 50, 70, 100, 150, 200],\n",
    "                \"max_depth\": [1, 2, 3, 4, 5],\n",
    "            })\n",
    "        param_grid = ParameterGrid({\n",
    "                \"n_estimators\": [30, 50, 70, 100],\n",
    "                \"max_depth\": [1, 2, 3],\n",
    "            })\n",
    "        \n",
    "        for param_set in param_grid:\n",
    "            scores.append({\n",
    "                    \"params\": param_set,\n",
    "                    \"score\": _score(\n",
    "                        trainidx=train,\n",
    "                        testidx=test,\n",
    "                        feats = _get_features(\n",
    "                            pcats=[cats.loc[catid, \"parentCategoryID\"]],\n",
    "                            cats=[catid]\n",
    "                        ),\n",
    "                        est=XGBClassifier(**param_set))\n",
    "                })\n",
    "            print scores[-1]\n",
    "        \n",
    "        scores = sorted(scores, key=lambda x: x[\"score\"], reverse=True)\n",
    "        for score in scores:\n",
    "            print score[\"score\"], score[\"params\"]\n",
    "        \n",
    "        newtest(\n",
    "            trainidx=fulltrain,\n",
    "            testidx=fulltest,\n",
    "            feats = _get_features(\n",
    "                pcats=[cats.loc[catid, \"parentCategoryID\"]],\n",
    "                cats=[catid]\n",
    "            ),\n",
    "            est=XGBClassifier(**scores[0][\"params\"])\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def _gentest():\n",
    "    import gc\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    from sklearn.grid_search import ParameterGrid\n",
    "    \n",
    "    prtrain = pd.read_csv(\"data/ItemPairs_train.csv\")\n",
    "    prtrain = pd.merge(\n",
    "        prtrain,\n",
    "        pd.read_csv(\"data/ItemInfo_train.csv_categoryID.csv\", index_col=0),\n",
    "        left_on=\"itemID_1\", right_index=True,\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "\n",
    "    prtest = pd.read_csv(\"data/ItemPairs_test.csv\", index_col=0)\n",
    "    prtest = pd.merge(\n",
    "        prtest,\n",
    "        pd.read_csv(\"data/ItemInfo_test.csv_categoryID.csv\", index_col=0),\n",
    "        left_on=\"itemID_1\", right_index=True,\n",
    "        how=\"inner\", sort=False\n",
    "    )\n",
    "    \n",
    "    cats = pd.read_csv(\"data/Category.csv\", index_col=0)\n",
    "    \n",
    "    for genmethod in np.unique(prtrain[\"generationMethod\"].values):\n",
    "        print \"GENERATION METHOD: %d\" % genmethod\n",
    "        os.environ[\"RESTRAIN\"] = \"/tmp/avito-gen-%d-train.csv.gz\" % genmethod\n",
    "        os.environ[\"RESTEST\"] = \"/tmp/avito-gen-%d-test.csv.gz\" % genmethod\n",
    "        if all((os.path.exists(os.environ[\"RESTRAIN\"]),\n",
    "                os.path.exists(os.environ[\"RESTEST\"]),\n",
    "                os.environ.get(\"RECALC\", None) is None)):\n",
    "            continue\n",
    "        \n",
    "        scores = []\n",
    "        fulltrain = shuffle(prtrain.index, random_state=42)\n",
    "        fittrain = shuffle(prtrain[prtrain[\"generationMethod\"] == genmethod].index, random_state=42)\n",
    "        fulltest = prtest.index\n",
    "        \n",
    "        newtest(\n",
    "            trainidx=fulltrain,\n",
    "            testidx=fulltest,\n",
    "            fitidx=fittrain\n",
    "        )\n",
    "        gc.collect()\n",
    "    \n",
    "\n",
    "def _catmerge():\n",
    "    cats = pd.read_csv(\"data/Category.csv\", index_col=0)\n",
    "    \n",
    "    _cattest()\n",
    "    \n",
    "    for catid in np.unique(cats.index):\n",
    "        print \"CATEGORY: %d\" % catid\n",
    "        with gzip.open(\"/tmp/avito-cat-%d-test.csv.gz\" % catid) as fi:\n",
    "            try:\n",
    "                prtest = pd.concat((\n",
    "                        prtest,\n",
    "                        pd.read_csv(fi, index_col=0)\n",
    "                    ), axis=0)\n",
    "            except NameError:\n",
    "                prtest = pd.read_csv(fi, index_col=0)\n",
    "    prtest.sort_index(inplace=True)\n",
    "    with gzip.open(\"/tmp/avito-catmerge.csv.gz\", \"wb\") as fo:\n",
    "        prtest[[\"probability\"]].to_csv(fo)\n",
    "\n",
    "\n",
    "def _pcatmerge():\n",
    "    cats = pd.read_csv(\"data/Category.csv\", index_col=0)\n",
    "    \n",
    "    _pcattest()\n",
    "    \n",
    "    for pcatid in np.unique(cats[\"parentCategoryID\"].values):\n",
    "        print \"PARENT CATEGORY: %d\" % pcatid\n",
    "        with gzip.open(\"/tmp/avito-pcat-%d-test.csv.gz\" % pcatid) as fi:\n",
    "            try:\n",
    "                prtest = pd.concat((\n",
    "                        prtest,\n",
    "                        pd.read_csv(fi, index_col=0)\n",
    "                    ), axis=0)\n",
    "            except NameError:\n",
    "                prtest = pd.read_csv(fi, index_col=0)\n",
    "    prtest.sort_index(inplace=True)\n",
    "    with gzip.open(\"/tmp/avito-pcatmerge.csv.gz\", \"wb\") as fo:\n",
    "        prtest[[\"probability\"]].to_csv(fo)\n",
    "\n",
    "\n",
    "def _newcatmerge():\n",
    "    cats = pd.read_csv(\"data/Category.csv\", index_col=0)\n",
    "    \n",
    "    _cattest()\n",
    "    \n",
    "    prtrain = pd.read_csv(\"data/ItemPairs_train.csv\")[[\"isDuplicate\"]]\n",
    "    prtrain = prtrain.loc[shuffle(prtrain.index, random_state=42)]\n",
    "    cats = pd.read_csv(\"data/Category.csv\", index_col=0)\n",
    "    \n",
    "    with gzip.open(\"/tmp/avito-train.csv.gz\") as fi:\n",
    "        prtrain[\"temp\"] = pd.read_csv(fi, index_col=0)[\"probability\"]\n",
    "        X = sp.csr_matrix(prtrain[[\"temp\"]].values.astype(float) - 0.5)\n",
    "\n",
    "    for catid in np.unique(cats.index):\n",
    "        print \"CATEGORY: %d\" % catid\n",
    "        with gzip.open(\"/tmp/avito-cat-%d-train.csv.gz\" % catid) as fi:\n",
    "            prtrain[\"temp\"] = pd.read_csv(fi, index_col=0)[\"probability\"].astype(float) - 0.5\n",
    "            X = sp.hstack((X, sp.csr_matrix(np.nan_to_num(prtrain[[\"temp\"]].values))))\n",
    "\n",
    "    print \"Shape:\", X.shape\n",
    "    est = Pipeline([\n",
    "            ('estimator', _get_estimator()),\n",
    "        ])\n",
    "\n",
    "    y = prtrain[\"isDuplicate\"].values\n",
    "    del prtrain\n",
    "    est.fit(X, y)\n",
    "    del X\n",
    "    del y\n",
    "\n",
    "    prtest = pd.read_csv(\"data/ItemPairs_test.csv\", index_col=0)[[]]\n",
    "    with gzip.open(\"/tmp/avito-test.csv.gz\") as fi:\n",
    "        prtest[\"temp\"] = pd.read_csv(fi, index_col=0)[\"probability\"]\n",
    "        X = sp.csr_matrix(prtest[[\"temp\"]].values.astype(float) - 0.5)\n",
    "\n",
    "    for catid in np.unique(cats.index):\n",
    "        print \"CATEGORY: %d\" % catid\n",
    "        with gzip.open(\"/tmp/avito-cat-%d-test.csv.gz\" % catid) as fi:\n",
    "            prtest[\"temp\"] = pd.read_csv(fi, index_col=0)[\"probability\"].astype(float) - 0.5\n",
    "            X = sp.hstack((X, sp.csr_matrix(np.nan_to_num(prtest[[\"temp\"]].values))))\n",
    "    prtest[\"probability\"] = est.predict_proba(X)[:,1]\n",
    "    del X\n",
    "    \n",
    "    prtest.sort_index(inplace=True)\n",
    "    with gzip.open(\"/tmp/avito-newcatmerge.csv.gz\", \"wb\") as fo:\n",
    "        prtest[[\"probability\"]].to_csv(fo)\n",
    "\n",
    "\n",
    "def _genmerge():\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    from sklearn.grid_search import ParameterGrid\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import get_scorer\n",
    "    from sklearn.svm import LinearSVC\n",
    "    \n",
    "    prtrain = pd.read_csv(\"data/ItemPairs_train.csv\")[[\"isDuplicate\"]]\n",
    "    prtrain = prtrain.loc[shuffle(prtrain.index, random_state=42)]\n",
    "    \n",
    "    with gzip.open(\"/tmp/avito-train.csv.gz\") as fi:\n",
    "        prtrain[\"temp\"] = pd.read_csv(fi, index_col=0)[\"probability\"]\n",
    "        X = sp.csr_matrix(prtrain[[\"temp\"]].values.astype(float))\n",
    "\n",
    "#     with gzip.open(\"/tmp/avito-linear-train.csv.gz\") as fi:\n",
    "#         prtrain[\"temp\"] = pd.read_csv(fi, index_col=0)[\"probability\"]\n",
    "#         X = sp.hstack((X, sp.csr_matrix(np.nan_to_num(prtrain[[\"temp\"]].values))))\n",
    "\n",
    "    for genmethod in (1, 3):\n",
    "        print \"METHOD: %d\" % genmethod\n",
    "        with gzip.open(\"/tmp/avito-gen-%d-train.csv.gz\" % genmethod) as fi:\n",
    "            prtrain[\"temp\"] = pd.read_csv(fi, index_col=0)[\"probability\"].astype(float)\n",
    "            X = sp.hstack((X, sp.csr_matrix(np.nan_to_num(prtrain[[\"temp\"]].values))))\n",
    "    \n",
    "    print \"Shape:\", X.shape\n",
    "    y = prtrain[\"isDuplicate\"].values\n",
    "    del prtrain\n",
    "    \n",
    "#     param_grid = ParameterGrid({\n",
    "#             \"n_estimators\": [1, 5, 10],\n",
    "#             \"max_depth\": [1, 2, 3, 5],\n",
    "#         })\n",
    "# #     param_grid = ParameterGrid({\n",
    "# #             \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "# #             \"loss\": [\"log\", \"modified_huber\"],\n",
    "# #         })\n",
    "# #     param_grid = ParameterGrid({\n",
    "# #             \"C\": [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "# #         })\n",
    "# #     param_grid = ParameterGrid({\n",
    "# #             \"C\": [1.0],\n",
    "# #         })\n",
    "    \n",
    "#     scores = []\n",
    "#     train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#     for params in param_grid:\n",
    "#         print params\n",
    "#         est = XGBClassifier(**params) # {'n_estimators': 10, 'max_depth': 2}\n",
    "# #         est = SGDClassifier(**params) # defaults\n",
    "# #         est = LogisticRegression(**params) # defaults \n",
    "# #         est = LinearSVC(**params)\n",
    "#         est.fit(train_x, train_y)\n",
    "#         print classification_report(test_y, est.predict(test_x))\n",
    "#         score = get_scorer(\"roc_auc\")(est, test_x, test_y)\n",
    "#         print score\n",
    "#         scores.append({\"params\": params, \"score\": score})\n",
    "    \n",
    "#     scores = sorted(scores, key=lambda x: x[\"score\"], reverse=True)\n",
    "#     for score in scores:\n",
    "#         print score\n",
    "    \n",
    "#     exit()\n",
    "    est = Pipeline([\n",
    "            ('estimator', Pipeline([\n",
    "                        (\"filter\", VarianceThreshold()),\n",
    "#                         (\"scaler\", StandardScaler(with_mean=False)),\n",
    "                        (\"est\", LogisticRegression())])),\n",
    "#             ('estimator', XGBClassifier(n_estimators=50, max_depth=2)),\n",
    "        ])\n",
    "\n",
    "    est.fit(X, y)\n",
    "    del X\n",
    "    del y\n",
    "\n",
    "    prtest = pd.read_csv(\"data/ItemPairs_test.csv\", index_col=0)[[]]\n",
    "    with gzip.open(\"/tmp/avito.csv.gz\") as fi:\n",
    "        prtest[\"temp\"] = pd.read_csv(fi, index_col=0)[\"probability\"]\n",
    "        X = sp.csr_matrix(prtest[[\"temp\"]].values.astype(float))\n",
    "\n",
    "#     with gzip.open(\"/tmp/avito-linear-test.csv.gz\") as fi:\n",
    "#         prtest[\"temp\"] = pd.read_csv(fi, index_col=0)[\"probability\"]\n",
    "#         X = sp.hstack((X, sp.csr_matrix(np.nan_to_num(prtest[[\"temp\"]].values))))\n",
    "\n",
    "    for genmethod in (1, 3):\n",
    "        print \"METHOD: %d\" % genmethod\n",
    "        with gzip.open(\"/tmp/avito-gen-%d-test.csv.gz\" % genmethod) as fi:\n",
    "            prtest[\"temp\"] = pd.read_csv(fi, index_col=0)[\"probability\"].astype(float)\n",
    "            X = sp.hstack((X, sp.csr_matrix(np.nan_to_num(prtest[[\"temp\"]].values))))\n",
    "    prtest[\"probability\"] = est.predict_proba(X)[:,1]\n",
    "    del X\n",
    "    \n",
    "    prtest.sort_index(inplace=True)\n",
    "    with gzip.open(\"/tmp/avito-genmerge.csv.gz\", \"wb\") as fo:\n",
    "        prtest[[\"probability\"]].to_csv(fo)\n",
    "\n",
    "\n",
    "def cattest():\n",
    "    import gzip\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.utils import shuffle\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    prtrain = pd.read_csv(\"data/ItemPairs_train.csv\")\n",
    "    prtest = pd.read_csv(\"data/ItemPairs_test.csv\", index_col=0)\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    indices = shuffle(prtrain.index, random_state=42)\n",
    "#     indices = shuffle(get_train_indices(\"categoryID\"), random_state=42)\n",
    "\n",
    "    est = MetaCategoryEstimator(\n",
    "        indices,\n",
    "        prtest.index,\n",
    "        [\n",
    "            ([\"pair_imagesdist_Correlation_%d\" % _], VarianceThreshold())\n",
    "            for _ in [95]\n",
    "        ] + [\n",
    "            ([\"pair_imagesdist_Chi-Squared_%d\" % _], VarianceThreshold())\n",
    "            for _ in [5]\n",
    "        ] + [\n",
    "            ([\"pair_imagesdist_Hellinger_%d\" % _], VarianceThreshold())\n",
    "            for _ in [5]\n",
    "        ] + [\n",
    "            ([\"pair_imagesdist_Intersection_%d\" % _], VarianceThreshold())\n",
    "            for _ in [230]\n",
    "        ] + [\n",
    "            ([\"title\"], SwitchTransformer(est=CountVectorizer(analyzer=\"char_wb\", ngram_range=(1,5), binary=True), preaction=\"ravel\", postaction=[\"and\", \"xor\"])),\n",
    "            ([\"description\"], SwitchTransformer(est=CountVectorizer(binary=True), preaction=\"ravel\", postaction=[\"and\", \"xor\"])),\n",
    "            ([\"attrsJSON\"], Pipeline([\n",
    "                        (\"tr1\", JsonToDict(1)),\n",
    "                        (\"tr2\", JsonAttrsTrf(1)),\n",
    "                        (\"imputer\", ConstantCol()),\n",
    "                    ])),\n",
    "            ([\"locationID\"], Pipeline([\n",
    "                        (\"binrz\", EqNonEqBinarizer()),\n",
    "                    ])),\n",
    "            ([\"regionID\"], Pipeline([\n",
    "                        (\"binrz\", EqNonEqBinarizer()),\n",
    "                    ])),\n",
    "            ([\"metroID\"], Pipeline([\n",
    "                        (\"binrz\", EqNonEqBinarizer()),\n",
    "                    ])),\n",
    "            ([\"price\"], Pipeline([\n",
    "                        (\"binrz\", EqNonEqBinarizer()),\n",
    "                    ])),\n",
    "            ([\"pair_imagesdist_Exact\"], VarianceThreshold()),\n",
    "            ([], Pipeline([(\"scaler\", StandardScaler(with_mean=False)), (\"est\", LogisticRegression(C=0.01))])),\n",
    "#             ([], XGBClassifier(n_estimators=100, max_depth=3)),\n",
    "        ],\n",
    "        [\n",
    "            ([\"pair_imagesdist_Correlation_%d\" % _], VarianceThreshold())\n",
    "            for _ in [90]\n",
    "        ] + [\n",
    "            ([\"pair_imagesdist_Chi-Squared_%d\" % _], VarianceThreshold())\n",
    "            for _ in [10]\n",
    "        ] + [\n",
    "            ([\"pair_imagesdist_Hellinger_%d\" % _], VarianceThreshold())\n",
    "            for _ in [10]\n",
    "        ] + [\n",
    "            ([\"pair_imagesdist_Intersection_%d\" % _], VarianceThreshold())\n",
    "            for _ in [200]\n",
    "        ] + [\n",
    "            ([\"title\"], SwitchTransformer(est=CountVectorizer(binary=True), preaction=\"ravel\", postaction=[\"and\", \"xor\"])),\n",
    "            ([\"description\"], SwitchTransformer(est=CountVectorizer(binary=True), preaction=\"ravel\", postaction=[\"and\", \"xor\"])),\n",
    "            ([\"attrsJSON\"], Pipeline([\n",
    "                        (\"tr1\", JsonToDict(1)),\n",
    "                        (\"tr2\", JsonAttrsTrf(1)),\n",
    "                        (\"imputer\", ConstantCol()),\n",
    "                    ])),\n",
    "            ([\"locationID\"], Pipeline([\n",
    "                        (\"binrz\", EqNonEqBinarizer()),\n",
    "                    ])),\n",
    "            ([\"regionID\"], Pipeline([\n",
    "                        (\"binrz\", EqNonEqBinarizer()),\n",
    "                    ])),\n",
    "            ([\"metroID\"], Pipeline([\n",
    "                        (\"binrz\", EqNonEqBinarizer()),\n",
    "                    ])),\n",
    "            ([\"price\"], Pipeline([\n",
    "                        (\"binrz\", EqNonEqBinarizer()),\n",
    "                    ])),\n",
    "            ([\"locationID\", \"lat\", \"lon\"], LocationDist(list(np.logspace(-3.5, 6, 20, base=2)))),\n",
    "            ([\"regionID\", \"lat\", \"lon\"], LocationDist(list(np.logspace(-2, 7, 20, base=2)))),\n",
    "            ([\"metroID\", \"lat\", \"lon\"], LocationDist(list(np.logspace(-4, 4, 5, base=2)))),\n",
    "            ([\"lat\", \"lon\"], CoordDist()),\n",
    "            ([\"categoryID\"], Pipeline([\n",
    "                        ('binrz', EqNonEqBinarizer()),\n",
    "                        ('filter', VarianceThreshold()),\n",
    "                    ])),\n",
    "            ([], Pipeline([(\"scaler\", StandardScaler(with_mean=False)), (\"est\", LogisticRegression(C=1.0))])),\n",
    "        ], None\n",
    "    )\n",
    "    est.fit(indices.values.reshape(-1,1), prtrain.loc[indices, \"isDuplicate\"].values)\n",
    "    \n",
    "    prtest[\"probability\"] = est.predict_proba(prtest.index.values.reshape(-1,1))[:,1]\n",
    "    with gzip.open(os.getenv(\"RESFILE\", \"/tmp/avito.csv.gz\"), \"wb\") as f:\n",
    "        prtest[[\"probability\"]].to_csv(f, float_format=\"%.7f\")\n",
    "\n",
    "    print \"TEST:\", (datetime.datetime.now() - start_time)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def _lintest():\n",
    "    os.environ[\"RESTRAIN\"] = \"/tmp/avito-linear-train.csv.gz\"\n",
    "    os.environ[\"RESTEST\"] = \"/tmp/avito-linear-test.csv.gz\"\n",
    "    newtest(\n",
    "        trainidx=shuffle(get_train_indices(\"all\"), random_state=42),\n",
    "        fitidx=shuffle(get_train_indices(), random_state=42),\n",
    "        feats=_get_features(linear=True),\n",
    "        est=LogisticRegression()\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "    if len(sys.argv) > 1 and sys.argv[-1] == \"test\":\n",
    "        newtest()\n",
    "    elif len(sys.argv) > 1 and sys.argv[-1] == \"cattest\":\n",
    "        _cattest()\n",
    "    elif len(sys.argv) > 1 and sys.argv[-1] == \"pcattest\":\n",
    "        _pcattest()\n",
    "    elif len(sys.argv) > 1 and sys.argv[-1] == \"gentest\":\n",
    "        _gentest()\n",
    "    elif len(sys.argv) > 1 and sys.argv[-1] == \"lintest\":\n",
    "        _lintest()\n",
    "    elif len(sys.argv) > 1 and sys.argv[-1] == \"catmerge\":\n",
    "        _catmerge()\n",
    "    elif len(sys.argv) > 1 and sys.argv[-1] == \"pcatmerge\":\n",
    "        _pcatmerge()\n",
    "    elif len(sys.argv) > 1 and sys.argv[-1] == \"newcatmerge\":\n",
    "        _newcatmerge()\n",
    "    elif len(sys.argv) > 1 and sys.argv[-1] == \"genmerge\":\n",
    "        _genmerge()\n",
    "    else:\n",
    "        print newcv(5).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
